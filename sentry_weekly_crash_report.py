#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sentry ì£¼ê°„ ë¦¬í¬íŠ¸ (ì§€ë‚œì£¼ ì›”~ì¼, í•œêµ­ì‹œê°„ ê¸°ì¤€)
- ì´ë²¤íŠ¸/ì´ìŠˆ/ì˜í–¥ ì‚¬ìš©ì ì£¼ê°„ í•©ê³„ + Crash Free(ì£¼ê°„ í‰ê· )
- ì´ë²¤íŠ¸ ê¸°ì¤€ ìƒìœ„ 5ê°œ ì´ìŠˆ (ì „ì£¼ ëŒ€ë¹„ë¥¼ ê°™ì€ ì¤„ì— í‘œê¸°)
- ì£¼ê°„ ì‹ ê·œ ë°œìƒ ì´ìŠˆ(ì„ íƒì  ì œí•œ)
- ì£¼ê°„ ê¸‰ì¦(ì„œì§€) ì´ìŠˆ (ê°„ë‹¨ ë² ì´ìŠ¤ë¼ì¸)
- ìµœì‹  ë¦´ë¦¬ì¦ˆ 1ê°œ(ì •ê·œ semver ìµœëŒ€) ê¸°ì¤€:
  â€¢ ì‚¬ë¼ì§„ ì´ìŠˆ (post 7ì¼ 0ê±´ + í˜„ì¬ resolved)
  â€¢ ë§ì´ ê°ì†Œí•œ ì´ìŠˆ (ì „í›„ 7ì¼ -80%p ì´ìƒ)
- Slack Webhookìœ¼ë¡œ ì „ì†¡ ê°€ëŠ¥

ë™ì‘ ê°€ì •:
- ì›”ìš”ì¼ ì˜¤ì „ 9ì‹œì— ì‹¤í–‰í•˜ì—¬ ì§€ë‚œì£¼(ì›”~ì¼) í†µê³„ë¥¼ ì „ë‹¬
"""

import json
import os
import re
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import quote_plus

import math
import requests
from dotenv import load_dotenv
from packaging.version import Version, InvalidVersion

API_BASE = "https://sentry.io/api/0"

# ====== íƒ€ì„ì¡´ ======
try:
    from zoneinfo import ZoneInfo  # py3.9+
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo  # type: ignore

KST = ZoneInfo("Asia/Seoul")
UTC = timezone.utc

# ====== ê³µí†µ ì„ê³„ì¹˜/í‘œì‹œ ìƒìˆ˜ ======
TITLE_MAX = 90
WEEKLY_TOP_LIMIT = 5
WEEKLY_NEW_LIMIT = 10
WEEKLY_SURGE_LIMIT = 10

# ì£¼ê°„ ì‹ ê·œ/í•´ê²°/ë¬µì€ ì´ìŠˆ ì„ê³„
WEEKLY_RESOLVED_MIN_EVENTS = 20
WEEKLY_RESOLVED_MIN_USERS  = 10

WEEKLY_STALE_MIN_AGE_DAYS  = 30
WEEKLY_STALE_MIN_EVENTS    = 5
WEEKLY_STALE_MIN_USERS     = 3
WEEKLY_STALE_LIMIT         = 20

# ê¸‰ì¦(ì„œì§€) ê°„ë‹¨ íŒì •
WEEKLY_SURGE_MIN_EVENTS         = 50
WEEKLY_SURGE_GROWTH_MULTIPLIER  = 2.0  # ì „ì£¼ ëŒ€ë¹„ 2ë°° ì´ìƒ
WEEKLY_SURGE_Z_THRESHOLD        = 2.0
WEEKLY_SURGE_MAD_THRESHOLD      = 3.5
WEEKLY_BASELINE_WEEKS           = 4    # ì§€ë‚œ 4ì£¼ ë² ì´ìŠ¤ë¼ì¸ (ì „ì£¼ í¬í•¨)

# ìµœì‹  ë¦´ë¦¬ì¦ˆ ì˜í–¥ ì¸¡ì •
RELEASE_FIX_IMPROVEMENT_DROP_PCT = 80.0   # -80%p ì´ìƒ ê°ì†Œ
RELEASE_FIXES_MIN_BASE_EVENTS     = 10    # ì „ 7ì¼ ìµœì†Œ ë² ì´ìŠ¤
WEEKLY_RELEASE_FIXES_PER_RELEASE_LIMIT = 20

# semver(+build) í—ˆìš©: 4.68.0 / 4.68.0+908 í—ˆìš©, 4.62.0.0-foo+20 ì œì™¸
SEMVER_RE = re.compile(r"^\d+\.\d+\.\d+(?:\+\d+)?$")

# Discover ê³µí†µ
LEVEL_QUERY = "level:[error,fatal]"

# =============== ë¡œê·¸ ìœ í‹¸ ===============
def wlog(msg: str) -> None:
    print(f"[Weekly] {msg}")

# =============== ìœ í‹¸ ===============
def bold(s: str) -> str:
    return f"*{s}*"

def truncate(s: Optional[str], n: int) -> str:
    if not s:
        return "(ì œëª© ì—†ìŒ)"
    return s if len(s) <= n else s[: n - 1] + "â€¦"

def ensure_ok(r: requests.Response) -> requests.Response:
    try:
        r.raise_for_status()
    except requests.HTTPError as e:
        msg = f"HTTP {r.status_code} for {r.request.method} {r.url}\nResponse: {r.text[:800]}"
        raise SystemExit(msg) from e
    return r

def auth_headers(token: str) -> Dict[str, str]:
    return {"Authorization": f"Bearer {token}"}

def date_kst(d: datetime) -> datetime:
    return d.astimezone(KST)

def to_utc_iso(dt: datetime) -> str:
    return dt.astimezone(UTC).isoformat().replace("+00:00","Z")

def parse_iso(s: str) -> datetime:
    return datetime.fromisoformat(s.replace("Z","+00:00"))

def kst_week_bounds_for_last_week(today_kst: datetime) -> Tuple[datetime, datetime]:
    """
    today_kst ê¸°ì¤€ 'ì§€ë‚œì£¼ ì›” 00:00:00' ~ 'ì§€ë‚œì£¼ ì¼ 23:59:59.999' ê²½ê³„ë¥¼ ë°˜í™˜
    - ì›”ìš”ì¼ ì‹¤í–‰ì„ ê°€ì •í•˜ì§€ë§Œ, ì–´ë–¤ ìš”ì¼ì— ì‹¤í–‰í•´ë„ 'ì™„ë£Œëœ ì§€ë‚œ ì£¼'ë¥¼ ê³ ì • ë°˜í™˜
    """
    dow = today_kst.weekday()  # Mon=0
    this_mon = today_kst.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=dow)
    last_mon = this_mon - timedelta(days=7)
    last_sun_end = this_mon - timedelta(microseconds=1)
    return last_mon, last_sun_end

def kst_week_bounds_for_prev_prev_week(today_kst: datetime) -> Tuple[datetime, datetime]:
    last_mon, last_sun_end = kst_week_bounds_for_last_week(today_kst)
    prev_last_mon = last_mon - timedelta(days=7)
    prev_last_sun_end = last_mon - timedelta(microseconds=1)
    return prev_last_mon, prev_last_sun_end

def pretty_kst_date(d: datetime) -> str:
    return d.strftime("%Y-%m-%d")

def pretty_kst_range(start_kst: datetime, end_kst: datetime) -> str:
    s = start_kst.strftime("%Y-%m-%d")
    e = end_kst.strftime("%Y-%m-%d")
    return f"{s} ~ {e} (KST)"

def diff_line(cur: int, prev: int, unit: str="ê±´") -> str:
    delta = cur - prev
    if delta > 0:
        arrow = ":small_red_triangle:"
    elif delta < 0:
        arrow = ":small_red_triangle_down:"
    else:
        arrow = "â€”"
    ratio = f" ({(delta/prev)*100:+.1f}%)" if prev > 0 else ""
    return f"{cur}{unit} -> ì „ì£¼ ëŒ€ë¹„: {arrow}{abs(delta)}{unit}{ratio}"

# ì¶”ê°€: ì»´íŒ©íŠ¸ ì „ì£¼ëŒ€ë¹„ í¬ë§· (í˜„ì¬ê°’ + ì¤‘ê°„ì  + ì¦ê°/í¼ì„¼íŠ¸)
def diff_compact(cur: int, prev: int, unit: str="ê±´") -> str:
    delta = cur - prev
    if delta > 0:
        arrow = ":small_red_triangle:"
    elif delta < 0:
        arrow = ":small_red_triangle_down:"
    else:
        arrow = "â€”"
    ratio = f" ({(delta/prev)*100:+.1f}%)" if prev > 0 else ""
    # '  Â· ' ì‚¬ì´ì— ê³µë°± ë‘ ì¹¸ + ì¤‘ê°„ì  ìœ ì§€ (ìš”ì²­ í¬ë§·)
    return f"{cur}{unit}  Â· {arrow}{abs(delta)}{unit}{ratio}"

# =========== HTTP í˜ì´ì§• ìœ í‹¸ ===========
def parse_next_cursor(link_header: str) -> Optional[str]:
    """
    Link í—¤ë”ì—ì„œ rel="next"ì˜ cursorë§Œ ì•ˆì „ ì¶”ì¶œ
    """
    if not link_header:
        return None
    parts = [p.strip() for p in link_header.split(",")]
    for p in parts:
        if 'rel="next"' not in p:
            continue
        if 'results="false"' in p:
            return None
        m = re.search(r'cursor="([^"]+)"', p)
        if m:
            cur = m.group(1)
            if ":-1:" in cur:
                return None
            return cur
        m2 = re.search(r'cursor=([^;>]+)', p)
        if m2:
            cur = m2.group(1)
            if ":-1:" in cur:
                return None
            return cur
    return None

# =========== í”„ë¡œì íŠ¸/ë¦´ë¦¬ì¦ˆ ===========
def resolve_project_id(token: str, org: str, project_slug: Optional[str], project_id_env: Optional[str]) -> int:
    if project_id_env:
        return int(project_id_env)
    if not project_slug:
        raise SystemExit("SENTRY_PROJECT_SLUG ë˜ëŠ” SENTRY_PROJECT_ID ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤.")
    wlog("[3/13] í”„ë¡œì íŠ¸ ID í™•ì¸ ì¤‘â€¦")
    url = f"{API_BASE}/organizations/{org}/projects/"
    r = ensure_ok(requests.get(url, headers=auth_headers(token), timeout=30))
    for p in r.json():
        if p.get("slug") == project_slug:
            pid = int(p.get("id"))
            wlog(f"[3/13] í”„ë¡œì íŠ¸ '{project_slug}' â†’ ID={pid}")
            return pid
    raise SystemExit(f"'{project_slug}' í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def list_releases_paginated(token: str, org: str, project_id: int, per_page: int=100, max_pages: int=20) -> List[Dict[str, Any]]:
    wlog("[10/13] ë¦´ë¦¬ì¦ˆ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘â€¦")
    url = f"{API_BASE}/organizations/{org}/releases/"
    headers = auth_headers(token)
    out: List[Dict[str, Any]] = []
    cursor: Optional[str] = None
    pages = 0
    while True:
        pages += 1
        params = {"project": project_id, "per_page": min(max(per_page,1),100)}
        if cursor:
            params["cursor"] = cursor
        r = ensure_ok(requests.get(url, headers=headers, params=params, timeout=60))
        data = r.json() or []
        out.extend(data)
        wlog(f"[10/13] ë¦´ë¦¬ì¦ˆ í˜ì´ì§€ {pages}: {len(data)}ê°œ")
        cursor = parse_next_cursor(r.headers.get("link",""))
        if not cursor or pages >= max_pages or not data:
            break
    wlog(f"[10/13] ë¦´ë¦¬ì¦ˆ ì´ {len(out)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return out

def latest_release_version(token: str, org: str, project_id: int) -> Optional[str]:
    wlog("[11/13] ìµœì‹  ë¦´ë¦¬ì¦ˆ(semver) ì„ íƒ ì‹œì‘â€¦")
    rels = list_releases_paginated(token, org, project_id)
    cands: List[Tuple[Version,str]] = []
    for r in rels:
        name = str(r.get("version") or r.get("shortVersion") or "").strip()
        if not name or not SEMVER_RE.match(name):
            continue
        try:
            base = name.split("+")[0]
            cands.append((Version(base), name))
        except InvalidVersion:
            continue
    if not cands:
        wlog("[11/13] ì •ê·œ semver ë¦´ë¦¬ì¦ˆ ì—†ìŒ")
        return None
    cands.sort(key=lambda x: x[0], reverse=True)
    best = cands[0][1]
    wlog(f"[11/13] ìµœì‹  ë¦´ë¦¬ì¦ˆ: {best}")
    return best

# =========== Discover/Issues ì§‘ê³„ ===========
def discover_aggregates(token: str, org: str, project_id: int, environment: Optional[str], start_iso: str, end_iso: str) -> Dict[str, int]:
    wlog("[4/13] ì£¼ê°„ í•©ê³„ ì§‘ê³„(ì´ë²¤íŠ¸/ì´ìŠˆ/ì‚¬ìš©ì)â€¦")
    url = f"{API_BASE}/organizations/{org}/events/"
    query = f"{LEVEL_QUERY}" + (f" environment:{environment}" if environment else "")
    params = {
        "field": ["count()", "count_unique(issue)", "count_unique(user)"],
        "project": project_id,
        "start": start_iso,
        "end": end_iso,
        "query": query,
        "referrer": "api.weekly.aggregates",
    }
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    rows = (r.json().get("data") or [])
    if not rows:
        wlog("  - ì§‘ê³„ ì—†ìŒ (0,0,0)")
        return {"events": 0, "issues": 0, "users": 0}
    row0 = rows[0]
    out = {
        "events": int(row0.get("count()") or 0),
        "issues": int(row0.get("count_unique(issue)") or 0),
        "users": int(row0.get("count_unique(user)") or 0),
    }
    wlog(f"  - events={out['events']} / issues={out['issues']} / users={out['users']}")
    return out

def sessions_crash_free_weekly_avg(token: str, org: str, project_id: int, environment: Optional[str], start_iso: str, end_iso: str) -> Tuple[Optional[float], Optional[float]]:
    wlog("[5/13] Crash Free(ì£¼ê°„ í‰ê· ) ì§‘ê³„â€¦")
    url = f"{API_BASE}/organizations/{org}/sessions/"
    params = {
        "project": project_id,
        "start": start_iso,
        "end": end_iso,
        "interval": "1d",
        "field": ["crash_free_rate(session)", "crash_free_rate(user)"],
        "referrer": "api.weekly.sessions",
    }
    if environment:
        params["environment"] = environment
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    data = r.json() or {}
    days = 0
    sum_s = 0.0
    sum_u = 0.0
    for g in data.get("groups", []):
        series = g.get("series", {})
        if "crash_free_rate(session)" in series:
            arr = series["crash_free_rate(session)"] or []
            if arr:
                sum_s += sum(arr)
                days = max(days, len(arr))
        if "crash_free_rate(user)" in series:
            arr = series["crash_free_rate(user)"] or []
            if arr:
                sum_u += sum(arr)
                days = max(days, len(arr))
    avg_s = (sum_s / days) if days > 0 else None
    avg_u = (sum_u / days) if days > 0 else None
    wlog(f"  - crash_free(session)={fmt_pct_trunc2(avg_s)} / crash_free(user)={fmt_pct_trunc2(avg_u)}")
    return avg_s, avg_u

def discover_issue_table(token: str, org: str, project_id: int, environment: Optional[str],
                         start_iso: str, end_iso: str, orderby: str="-count()", limit: int=50) -> List[Dict[str, Any]]:
    url = f"{API_BASE}/organizations/{org}/events/"
    query = f"{LEVEL_QUERY}" + (f" environment:{environment}" if environment else "")
    params = {
        "field": ["issue.id", "issue", "title", "count()", "count_unique(user)"],
        "project": project_id,
        "start": start_iso,
        "end": end_iso,
        "query": query,
        "orderby": orderby,
        "per_page": min(max(limit, 1), 100),
        "referrer": "api.weekly.issue-table",
    }
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    rows = r.json().get("data") or []
    out = []
    for row in rows[:limit]:
        iid_num = str(row.get("issue.id") or "")
        short = row.get("issue")
        out.append({
            "issue_id": iid_num,
            "short_id": short,
            "title": row.get("title"),
            "events": int(row.get("count()") or 0),
            "users": int(row.get("count_unique(user)") or 0),
            "link": f"https://sentry.io/organizations/{org}/issues/{iid_num}/" if iid_num else None,
        })
    return out

def issues_search(token: str, org: str, project_id: int, query: str, since_iso: Optional[str], until_iso: Optional[str], per_page: int=100) -> List[Dict[str, Any]]:
    url = f"{API_BASE}/organizations/{org}/issues/"
    headers = auth_headers(token)
    params = {
        "project": project_id,
        "query": query,
        "per_page": min(max(per_page,1),100),
        "referrer": "api.weekly.issues-search",
    }
    if since_iso: params["since"] = since_iso
    if until_iso: params["until"] = until_iso
    out: List[Dict[str, Any]] = []
    cursor: Optional[str] = None
    pages = 0
    while True:
        pages += 1
        if cursor:
            params["cursor"] = cursor
        r = ensure_ok(requests.get(url, headers=headers, params=params, timeout=60))
        arr = r.json() or []
        out.extend(arr)
        cursor = parse_next_cursor(r.headers.get("link",""))
        if not cursor or not arr:
            break
    return out

def count_for_issues_in_window(token: str, org: str, project_id: int, environment: Optional[str],
                               issue_ids: List[str], start_iso: str, end_iso: str) -> Dict[str, Dict[str,int]]:
    if not issue_ids:
        return {}
    url = f"{API_BASE}/organizations/{org}/events/"
    query = f"{LEVEL_QUERY}" + (f" environment:{environment}" if environment else "")
    params = {
        "field": ["issue.id", "count()", "count_unique(user)"],
        "project": project_id,
        "start": start_iso,
        "end": end_iso,
        "query": query + f" issue.id:[{','.join(issue_ids)}]",
        "orderby": "-count()",
        "per_page": 100,
        "referrer": "api.weekly.issue-bulk-counts",
    }
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    rows = r.json().get("data") or []
    out: Dict[str, Dict[str,int]] = {}
    for row in rows:
        iid = str(row.get("issue.id"))
        out[iid] = {
            "events": int(row.get("count()") or 0),
            "users": int(row.get("count_unique(user)") or 0),
        }
    return out

# =========== í†µê³„ ìœ í‹¸ ===========
def mean_std(values: List[float]) -> Tuple[float, float]:
    if not values:
        return 0.0, 0.0
    m = sum(values)/len(values)
    var = sum((v-m)**2 for v in values) / len(values)
    return m, math.sqrt(var)

def median(values: List[float]) -> float:
    if not values:
        return 0.0
    s = sorted(values)
    n = len(s)
    mid = n//2
    if n % 2 == 1:
        return float(s[mid])
    return (s[mid-1] + s[mid]) / 2.0

def mad(values: List[float], med: Optional[float]=None) -> float:
    if not values:
        return 0.0
    m = med if med is not None else median(values)
    dev = [abs(v - m) for v in values]
    return median(dev)

# =========== ì£¼ê°„ ê¸°ëŠ¥: ì‹ ê·œ/ê¸‰ì¦/í•´ê²°/ë¬µì€ ===========
def new_issues_in_week(token: str, org: str, project_id: int, environment: Optional[str], start_iso: str, end_iso: str, limit: int=WEEKLY_NEW_LIMIT) -> List[Dict[str, Any]]:
    wlog("[7/13] ì£¼ê°„ ì‹ ê·œ ë°œìƒ ì´ìŠˆ ìˆ˜ì§‘â€¦")
    q = [LEVEL_QUERY, f"firstSeen:>={start_iso}", f"firstSeen:<{end_iso}"]
    if environment:
        q.append(f"environment:{environment}")
    items = issues_search(token, org, project_id, " ".join(q), since_iso=start_iso, until_iso=end_iso, per_page=100)
    out = []
    for it in items[:limit]:
        iid = it.get("id")
        out.append({
            "issue_id": iid,
            "title": it.get("title"),
            "count": int(it.get("count") or 0),
            "first_seen": it.get("firstSeen"),
            "link": it.get("permalink") or (f"https://sentry.io/organizations/{org}/issues/{iid}/" if iid else None)
        })
    wlog(f"[7/13] ì‹ ê·œ ì´ìŠˆ {len(out)}ê°œ")
    return out

def detect_weekly_surge(token: str, org: str, project_id: int, environment: Optional[str],
                        this_start_iso: str, this_end_iso: str, prev_start_iso: str, prev_end_iso: str) -> List[Dict[str, Any]]:
    """
    ê°„ë‹¨ ì£¼ê°„ ì„œì§€: ì´ë²ˆì£¼ vs ì „ì£¼, + ì§€ë‚œ 4ì£¼(ì „ì£¼ í¬í•¨) ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ Z/MAD ì²´í¬
    """
    wlog("[7/13] ì£¼ê°„ ê¸‰ì¦(ì„œì§€) ì´ìŠˆ íƒì§€â€¦")
    this_top = discover_issue_table(token, org, project_id, environment, this_start_iso, this_end_iso, "-count()", 100)
    prev_top = discover_issue_table(token, org, project_id, environment, prev_start_iso, prev_end_iso, "-count()", 100)

    this_map = {str(x["issue_id"]): x for x in this_top}
    prev_map = {str(x["issue_id"]): x for x in prev_top}

    # ë² ì´ìŠ¤ë¼ì¸: ì§€ë‚œ 4ì£¼(ì „ì£¼ í¬í•¨)ì˜ weekly events
    baselines: Dict[str, List[int]] = {iid: [] for iid in set(list(this_map.keys()) + list(prev_map.keys()))}

    # ì´ë²ˆì£¼ ê¸°ì¤€ ì¢…ë£Œì¼ì˜ ì§ì „ ì£¼ë¶€í„° 4ì£¼ ìˆ˜ì§‘
    end_dt = parse_iso(this_end_iso)
    for w in range(1, WEEKLY_BASELINE_WEEKS+1):
        w_end = (end_dt - timedelta(days=7*w))
        w_start = w_end - timedelta(days=6, hours=23, minutes=59, seconds=59, microseconds=999999)
        w_s_iso = to_utc_iso(w_start)
        w_e_iso = to_utc_iso(w_end)
        rows = discover_issue_table(token, org, project_id, environment, w_s_iso, w_e_iso, "-count()", 200)
        wmap = {str(r["issue_id"]): r for r in rows}
        for iid in baselines.keys():
            baselines[iid].append(int(wmap.get(iid, {}).get("events", 0)))

    out: List[Dict[str, Any]] = []
    for iid, it in this_map.items():
        cur = int(it.get("events", 0))
        if cur < WEEKLY_SURGE_MIN_EVENTS:
            continue
        prev = int(prev_map.get(iid, {}).get("events", 0))
        growth = (cur / max(prev, 1))
        base_vals = [float(x) for x in baselines.get(iid, []) if isinstance(x,(int,float))]
        m, s = mean_std(base_vals)
        med = median(base_vals)
        m_mad = mad(base_vals, med)
        eps = 1e-9
        z = (cur - m) / (s + eps) if s > 0 else (float("inf") if cur > m else 0.0)
        mad_s = (cur - med) / (1.4826 * m_mad + eps) if m_mad > 0 else (float("inf") if cur > med else 0.0)

        conds = {
            "growth": growth >= WEEKLY_SURGE_GROWTH_MULTIPLIER,
            "zscore": z >= WEEKLY_SURGE_Z_THRESHOLD,
            "madscore": mad_s >= WEEKLY_SURGE_MAD_THRESHOLD,
        }
        if any(conds.values()):
            out.append({
                "issue_id": iid,
                "title": it.get("title"),
                "event_count": cur,
                "prev_count": prev,
                "growth_multiplier": round(growth, 2),
                "zscore": None if math.isinf(z) else round(z, 2),
                "mad_score": None if math.isinf(mad_s) else round(mad_s, 2),
                "link": it.get("link"),
                "reasons": [k for k,v in conds.items() if v]
            })
    out.sort(key=lambda x: (x["event_count"], (x["zscore"] or 0), (x["mad_score"] or 0), x["growth_multiplier"]), reverse=True)
    wlog(f"[7/13] ê¸‰ì¦ ì´ìŠˆ {len(out)}ê°œ")
    return out[:WEEKLY_SURGE_LIMIT]

def fetch_issue_detail(token: str, org: str, issue_key: str) -> Dict[str, Any]:
    """
    issue_keyê°€ ìˆ«ìí˜•ì´ë©´ ë°”ë¡œ /issues/{id}/
    ìˆ«ìê°€ ì•„ë‹ˆë©´ shortIdë¡œ ê²€ìƒ‰í•´ì„œ ìˆ«ìí˜• idë¡œ ì¬ì¡°íšŒ
    """
    headers = auth_headers(token)

    if issue_key.isdigit():
        url = f"{API_BASE}/issues/{issue_key}/"
        r = ensure_ok(requests.get(url, headers=headers, timeout=30))
        return r.json() or {}

    search_url = f"{API_BASE}/organizations/{org}/issues/"
    params = {
        "query": f"shortId:{issue_key}",
        "per_page": 1,
        "referrer": "api.weekly.issue-detail-resolve",
    }
    r = ensure_ok(requests.get(search_url, headers=headers, params=params, timeout=30))
    arr = r.json() or []
    if not arr:
        raise SystemExit(f"ì´ìŠˆ shortId '{issue_key}'ë¥¼ ìˆ«ìí˜• IDë¡œ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    numeric_id = str(arr[0].get("id") or "")
    if not numeric_id:
        raise SystemExit(f"shortId '{issue_key}' ì‘ë‹µì— ìˆ«ìí˜• idê°€ ì—†ìŠµë‹ˆë‹¤.")

    url = f"{API_BASE}/issues/{numeric_id}/"
    r = ensure_ok(requests.get(url, headers=headers, timeout=30))
    return r.json() or {}

def release_fixes_in_week(token: str, org: str, project_id: int, environment: Optional[str],
                          week_start_iso: str, week_end_iso: str) -> List[Dict[str, Any]]:
    """
    ìµœì‹ (semver ìµœëŒ“ê°’) ë¦´ë¦¬ì¦ˆ 1ê°œë§Œ ëŒ€ìƒìœ¼ë¡œ ì „/í›„ 7ì¼ ë¹„êµ:
      - ì‚¬ë¼ì§„ ì´ìŠˆ: post 7ì¼ 0ê±´ AND í˜„ì¬ status=resolved
      - ë§ì´ ê°ì†Œí•œ ì´ìŠˆ: post>0 AND ì „í›„ -80%p ì´ìƒ ê°ì†Œ
    """
    wlog("[12/13] ìµœì‹  ë¦´ë¦¬ì¦ˆ ê°œì„  ê°ì§€ ì‹œì‘â€¦")
    best_rel = latest_release_version(token, org, project_id)
    if not best_rel:
        wlog("[12/13] ìµœì‹  ë¦´ë¦¬ì¦ˆ ì—†ìŒ")
        return []

    wlog("[12/13] ì „í›„ ë¹„êµ ëŒ€ìƒ(ì´ë²¤íŠ¸ Top50) ìˆ˜ì§‘â€¦")
    top_e = discover_issue_table(token, org, project_id, environment, week_start_iso, week_end_iso, "-count()", 50)
    pool = {it["issue_id"]: it for it in top_e if it.get("issue_id")}
    ids = list(pool.keys())
    if not pool:
        wlog("[12/13] ë¹„êµ ëŒ€ìƒ ì—†ìŒ")
        return [{"release": best_rel, "disappeared": [], "decreased": []}]

    week_end_dt = parse_iso(week_end_iso)
    pivot = week_end_dt - timedelta(days=1)
    pre_start = to_utc_iso(pivot - timedelta(days=7))
    pre_end   = to_utc_iso(pivot)
    post_start= to_utc_iso(pivot + timedelta(seconds=1))
    post_end  = to_utc_iso(pivot + timedelta(days=7))

    wlog("[12/13] ì „ê¸°ê°„ ì§‘ê³„â€¦")
    pre_map  = count_for_issues_in_window(token, org, project_id, environment, ids, pre_start, pre_end)
    wlog("[12/13] í›„ê¸°ê°„ ì§‘ê³„â€¦")
    post_map = count_for_issues_in_window(token, org, project_id, environment, ids, post_start, post_end)

    disappeared: List[Dict[str, Any]] = []
    decreased:   List[Dict[str, Any]] = []

    wlog("[12/13] ì „/í›„ ë¹„êµ íŒì •â€¦")
    for iid in ids:
        pre_ev  = int(pre_map.get(iid, {}).get("events", 0))
        post_ev = int(post_map.get(iid, {}).get("events", 0))
        if pre_ev < RELEASE_FIXES_MIN_BASE_EVENTS:
            continue
        status = None
        try:
            status = (fetch_issue_detail(token, iid).get("status") or "").lower()
        except Exception:
            pass
        drop_pct = 100.0*(pre_ev - post_ev)/pre_ev if pre_ev>0 else 0.0

        if post_ev == 0 and status == "resolved":
            disappeared.append({
                "issue_id": iid,
                "title": pool[iid].get("title"),
                "pre_7d_events": pre_ev,
                "post_7d_events": post_ev,
                "link": pool[iid].get("link"),
            })
            continue

        if post_ev > 0 and drop_pct >= RELEASE_FIX_IMPROVEMENT_DROP_PCT:
            decreased.append({
                "issue_id": iid,
                "title": pool[iid].get("title"),
                "pre_7d_events": pre_ev,
                "post_7d_events": post_ev,
                "delta_pct": round(-drop_pct, 1),
                "link": pool[iid].get("link"),
            })

    disappeared.sort(key=lambda x: x["pre_7d_events"], reverse=True)
    decreased.sort(key=lambda x: (x["delta_pct"], -x["post_7d_events"]), reverse=True)
    wlog(f"[12/13] ìµœì‹  '{best_rel}' â†’ ì‚¬ë¼ì§„:{len(disappeared)} / ê°ì†Œ:{len(decreased)}")
    return [{
        "release": best_rel,
        "disappeared": disappeared[:WEEKLY_RELEASE_FIXES_PER_RELEASE_LIMIT],
        "decreased":   decreased[:WEEKLY_RELEASE_FIXES_PER_RELEASE_LIMIT],
    }]

def build_sentry_action_urls(
    org: str,
    project_id: int,
    environment: Optional[str],
    start_iso_utc: str,
    end_iso_utc: str,
) -> Dict[str, str]:
    """
    - dashboard_url: SENTRY_DASHBOARD_URL > DASH_BOARD_ID > ì¡°ì§ í”„ë¡œì íŠ¸ ëª©ë¡ ìˆœìœ¼ë¡œ ì„ íƒ
    - issues_filtered_url: ë¶„ì„ êµ¬ê°„(start/end) + level(error,fatal) + environment ì¿¼ë¦¬ê°€ ì ìš©ëœ ì´ìŠˆ ëª©ë¡
    """
    # 1) ëŒ€ì‹œë³´ë“œ URL
    env_dash = os.getenv("SENTRY_DASHBOARD_URL")
    dash_id  = os.getenv("DASH_BOARD_ID")
    if env_dash:
        dashboard_url = env_dash
    elif dash_id:
        dashboard_url = f"https://sentry.io/organizations/{org}/dashboard/{dash_id}/?project={project_id}"
        # (ì›í•˜ì‹œë©´ ì„œë¸Œë„ë©”ì¸ ìŠ¤íƒ€ì¼ë„ ê°€ëŠ¥: f"https://{org}.sentry.io/dashboard/{dash_id}/?project={project_id}")
    else:
        dashboard_url = f"https://sentry.io/organizations/{org}/projects/"

    # 2) ì´ìŠˆ ëª©ë¡ URL (organizations ê²½ë¡œ + query + start/end)
    base = f"https://sentry.io/organizations/{org}/issues/"
    q_parts = ["level:[error,fatal]"]
    if environment:
        q_parts.append(f"environment:{environment}")
    q = quote_plus(" ".join(q_parts))
    s = quote_plus(start_iso_utc)
    e = quote_plus(end_iso_utc)
    issues_filtered_url = f"{base}?project={project_id}&query={q}&start={s}&end={e}"

    return {
        "dashboard_url": dashboard_url,
        "issues_filtered_url": issues_filtered_url,
    }

# =========== Slack ë Œë”ë§ ===========
def fmt_pct_trunc2(v: Optional[float]) -> str:
    if v is None:
        return "N/A"
    pct = v*100.0
    truncated = int(pct*100)/100
    return f"{truncated:.2f}%"

# ê¸°ì¡´ í•¨ìˆ˜ ìˆ˜ì •
def issue_line_with_prev(item: Dict[str, Any], prev_map: Dict[str, Any]) -> str:
    """
    ì˜ˆ)
    â€¢ ì œëª© Â· 24ê±´ Â· 12ëª…  -> ì „ì£¼ ëŒ€ë¹„: :small_red_triangle_down:6ê±´ (-20.0%)
    """
    title = truncate(item.get("title"), TITLE_MAX)
    link  = item.get("link")
    ev    = int(item.get("events", 0))
    us    = int(item.get("users", 0))
    head  = f"â€¢ <{link}|{title}> Â· {ev}ê±´ Â· {us}ëª…" if link else f"â€¢ {title} Â· {ev}ê±´ Â· {us}ëª…"

    prev_ev = int(prev_map.get(str(item.get("issue_id")), {}).get("events", 0))
    tail = f" -> ì „ì£¼ ëŒ€ë¹„: {diff_delta_only(ev, prev_ev, 'ê±´')}"
    return head + " " + tail

def diff_delta_only(cur: int, prev: int, unit: str="ê±´") -> str:
    delta = cur - prev
    if delta > 0:
        arrow = ":small_red_triangle:"
    elif delta < 0:
        arrow = ":small_red_triangle_down:"
    else:
        arrow = "â€”"
    ratio = f" ({(delta/prev)*100:+.1f}%)" if prev > 0 else ""
    return f"{arrow}{abs(delta)}{unit}{ratio}"

def surge_reason_ko(reasons: List[str]) -> str:
    ko = {
        "growth": "ì „ì£¼ ëŒ€ë¹„ ê¸‰ì¦",
        "zscore": "í‰ê·  ëŒ€ë¹„ í†µê³„ì  ê¸‰ì¦",
        "madscore": "ì¤‘ì•™ê°’ ëŒ€ë¹„ ì´ìƒì¹˜",
    }
    labeled = [ko.get(x, x) for x in reasons]
    return "/".join(labeled)

def build_weekly_blocks(
    payload: Dict[str, Any],
    slack_title: str,
    env_label: Optional[str],
    org: str,
    project_id: int,
    week_window_utc: Dict[str, str],
) -> List[Dict[str, Any]]:
    blocks: List[Dict[str, Any]] = []
    blocks.append({"type":"header","text":{"type":"plain_text","text": slack_title, "emoji": True}})

    sum_this = payload.get("this_week", {})
    sum_prev = payload.get("prev_week", {})

    events = int(sum_this.get("events", 0))
    issues = int(sum_this.get("issues", 0))
    users  = int(sum_this.get("users", 0))
    prev_events = int(sum_prev.get("events", 0))
    prev_issues = int(sum_prev.get("issues", 0))
    prev_users  = int(sum_prev.get("users", 0))

    cf_s = sum_this.get("crash_free_sessions")
    cf_u = sum_this.get("crash_free_users")

    summary_lines = [
        bold(":memo: Summary"),
        f"â€¢ ğŸ’¥ *ì´ ì´ë²¤íŠ¸ ë°œìƒ ê±´ìˆ˜*: {diff_line(events, prev_events, 'ê±´')}",
        f"â€¢ ğŸ *ìœ ë‹ˆí¬ ì´ìŠˆ ê°œìˆ˜*: {diff_line(issues, prev_issues, 'ê°œ')}",
        f"â€¢ ğŸ‘¥ *ì˜í–¥ ì‚¬ìš©ì*: {diff_line(users, prev_users, 'ëª…')}",
        f"â€¢ ğŸ›¡ï¸ *Crash Free ì„¸ì…˜(ì£¼ê°„ í‰ê· )*: {fmt_pct_trunc2(cf_s)} / *Crash Free ì‚¬ìš©ì*: {fmt_pct_trunc2(cf_u)}",
    ]
    blocks.append({"type":"section","text":{"type":"mrkdwn","text":"\n".join(summary_lines)}})

    blocks.append({"type":"context","elements":[{"type":"mrkdwn","text": f"*ì§‘ê³„ êµ¬ê°„*: {payload.get('this_week_range_kst','?')}"}]})
    if env_label:
        blocks.append({"type":"context","elements":[{"type":"mrkdwn","text": f"*í™˜ê²½*: {env_label}"}]})
    blocks.append({"type":"divider"})

    top_this = payload.get("top5_events", [])
    prev_map = { str(x.get("issue_id")): x for x in payload.get("prev_top_events", []) }
    if top_this:
        blocks.append({"type":"section","text":{"type":"mrkdwn","text": bold(":sports_medal: ìƒìœ„ 5ê°œ ì´ìŠˆ(ì´ë²¤íŠ¸)")}})
        lines = [issue_line_with_prev(x, prev_map) for x in top_this[:WEEKLY_TOP_LIMIT]]
        blocks.append({"type":"section","text":{"type":"mrkdwn","text":"\n".join(lines)}})
        blocks.append({"type":"divider"})

    new_items = payload.get("new_issues", [])
    if new_items:
        blocks.append({"type":"section","text":{"type":"mrkdwn","text": bold(":new: ì£¼ê°„ ì‹ ê·œ ë°œìƒ ì´ìŠˆ")}})
        lines = [f"â€¢ <{x.get('link')}|{truncate(x.get('title'), TITLE_MAX)}> Â· {x.get('count',0)}ê±´" for x in new_items]
        blocks.append({"type":"section","text":{"type":"mrkdwn","text":"\n".join(lines)}})
        blocks.append({"type":"divider"})

    surges = payload.get("surge_issues", [])
    if surges:
        blocks.append({"type":"section","text":{"type":"mrkdwn","text": bold(":chart_with_upwards_trend: ê¸‰ì¦(ì„œì§€) ì´ìŠˆ")}})
        lines = []
        for s in surges:
            head = f"â€¢ <{s.get('link')}|{truncate(s.get('title'), TITLE_MAX)}> Â· {s.get('event_count',0)}ê±´"
            tail = f"  â†³ ì „ì£¼ {s.get('prev_count',0)}ê±´ â†’ ì´ë²ˆì£¼ {s.get('event_count',0)}ê±´. íŒì • ê·¼ê±°: {surge_reason_ko(s.get('reasons',[]))}"
            lines.append(head+"\n"+tail)
        blocks.append({"type":"section","text":{"type":"mrkdwn","text":"\n".join(lines)}})
        blocks.append({"type":"divider"})

    rfix = payload.get("this_week_release_fixes") or []
    if rfix:
        grp = rfix[0]
        blocks.append({"type":"section","text":{"type":"mrkdwn","text": bold("ğŸ“¦ ìµœì‹  ë¦´ë¦¬ì¦ˆì—ì„œ í•´ì†Œëœ ì´ìŠˆ")}})
        blocks.append({"type":"section","text":{"type":"mrkdwn","text": bold(f"â€¢ {grp.get('release')}")}})

        disappeared = grp.get("disappeared") or []
        decreased   = grp.get("decreased") or []

        if disappeared:
            rows = [bold("  â—¦ ì‚¬ë¼ì§„ ì´ìŠˆ(ì „í›„ 7ì¼ ë¹„êµ: 0ê±´ & í˜„ì¬ Resolved)")]
            for it in disappeared:
                rows.append(f"    â€¢ <{it.get('link')}|{truncate(it.get('title'), TITLE_MAX)}> â€” ì „ 7ì¼ {it.get('pre_7d_events')}ê±´ â†’ í›„ 7ì¼ 0ê±´")
            blocks.append({"type":"section","text":{"type":"mrkdwn","text":"\n".join(rows)}})

        if decreased:
            rows = [bold("  â—¦ ë§ì´ ê°ì†Œí•œ ì´ìŠˆ(ì „í›„ 7ì¼ -80%p ì´ìƒ)")]
            for it in decreased:
                rows.append(f"    â€¢ <{it.get('link')}|{truncate(it.get('title'), TITLE_MAX)}> â€” ì „ 7ì¼ {it.get('pre_7d_events')}ê±´ â†’ í›„ 7ì¼ {it.get('post_7d_events')}ê±´ ({it.get('delta_pct')}pp)")
            blocks.append({"type":"section","text":{"type":"mrkdwn","text":"\n".join(rows)}})

        blocks.append({"type":"divider"})

    try:
        actions_block = build_footer_actions_block(org, int(project_id), env_label, week_window_utc)
        blocks.append(actions_block)
    except Exception:
        # ì•¡ì…˜ ë¸”ë¡ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
        pass

    return blocks

def build_footer_actions_block(
    org: str,
    project_id: int,
    env_label: Optional[str],
    win: Dict[str, str],
) -> Dict[str, Any]:
    """
    Slack í•˜ë‹¨ ë²„íŠ¼ 2ê°œ:
    - ğŸ“Š ëŒ€ì‹œë³´ë“œ
    - ğŸ” í•´ë‹¹ ê¸°ê°„ ì´ìŠˆ ë³´ê¸° (level:error,fatal + environment + start/end)
    """
    start_iso = win.get("start", "")
    end_iso   = win.get("end", "")
    urls = build_sentry_action_urls(org, project_id, env_label, start_iso, end_iso)

    return {
        "type": "actions",
        "elements": [
            {
                "type": "button",
                "text": {"type": "plain_text", "text": "ğŸ“Š ëŒ€ì‹œë³´ë“œ"},
                "url": urls["dashboard_url"],
            },
            {
                "type": "button",
                "text": {"type": "plain_text", "text": "ğŸ” í•´ë‹¹ ê¸°ê°„ ì´ìŠˆ ë³´ê¸°"},
                "url": urls["issues_filtered_url"],
            },
        ],
    }

def post_to_slack(webhook_url: str, blocks: List[Dict[str, Any]]) -> None:
    payload = {"blocks": blocks}
    r = requests.post(webhook_url, headers={"Content-Type":"application/json"}, data=json.dumps(payload), timeout=30)
    try:
        r.raise_for_status()
        wlog("[13/13] Slack ì „ì†¡ ì™„ë£Œ.")
    except requests.HTTPError as e:
        print(f"[Slack] Post failed {r.status_code}: {r.text[:300]}")
        raise

# =========== ë©”ì¸ ===========
def main():
    step_total = 13
    wlog(f"[1/{step_total}] í™˜ê²½ ë¡œë“œâ€¦")
    load_dotenv()
    token = os.getenv("SENTRY_AUTH_TOKEN") or ""
    org = os.getenv("SENTRY_ORG_SLUG") or ""
    project_slug = os.getenv("SENTRY_PROJECT_SLUG")
    project_id_env = os.getenv("SENTRY_PROJECT_ID")
    environment = os.getenv("SENTRY_ENVIRONMENT")
    slack_webhook = os.getenv("SLACK_WEBHOOK_URL")

    if not token or not org:
        raise SystemExit("SENTRY_AUTH_TOKEN, SENTRY_ORG_SLUG í•„ìˆ˜")

    now_kst = datetime.now(KST)
    wlog(f"[2/{step_total}] ì£¼ê°„ ë²”ìœ„ ê³„ì‚°â€¦")
    this_start_kst, this_end_kst = kst_week_bounds_for_last_week(now_kst)
    prev_start_kst, prev_end_kst = kst_week_bounds_for_prev_prev_week(now_kst)

    this_start_iso = to_utc_iso(this_start_kst)
    this_end_iso   = to_utc_iso(this_end_kst)
    prev_start_iso = to_utc_iso(prev_start_kst)
    prev_end_iso   = to_utc_iso(prev_end_kst)

    this_range_label = pretty_kst_range(this_start_kst, this_end_kst)
    prev_range_label = pretty_kst_range(prev_start_kst, prev_end_kst)
    wlog(f"  - ì§€ë‚œì£¼: {this_range_label}")
    wlog(f"  - ì§€ì§€ë‚œì£¼: {prev_range_label}")

    wlog(f"[3/{step_total}] í”„ë¡œì íŠ¸ ID í™•ì¸â€¦")
    project_id = resolve_project_id(token, org, project_slug, project_id_env)

    # ì£¼ê°„ í•©ê³„
    this_sum = discover_aggregates(token, org, project_id, environment, this_start_iso, this_end_iso)
    prev_sum = discover_aggregates(token, org, project_id, environment, prev_start_iso, prev_end_iso)

    # Crash Free ì£¼ê°„ í‰ê· 
    cf_s, cf_u = sessions_crash_free_weekly_avg(token, org, project_id, environment, this_start_iso, this_end_iso)

    # ìƒìœ„ ì´ìŠˆ (ì´ë²¤íŠ¸ ê¸°ì¤€)
    wlog(f"[6/{step_total}] ìƒìœ„ ì´ìŠˆ(ì´ë²¤íŠ¸ Top5) ìˆ˜ì§‘â€¦")
    top_events_this = discover_issue_table(token, org, project_id, environment, this_start_iso, this_end_iso, "-count()", 50)
    top_events_prev = discover_issue_table(token, org, project_id, environment, prev_start_iso, prev_end_iso, "-count()", 50)
    wlog(f"  - ì´ë²ˆ ì£¼ Top í›„ë³´ {len(top_events_this)}ê°œ / ì „ì£¼ {len(top_events_prev)}ê°œ")

    # ì‹ ê·œ ì´ìŠˆ
    new_items = new_issues_in_week(token, org, project_id, environment, this_start_iso, this_end_iso)

    # ê¸‰ì¦ ì´ìŠˆ(ì£¼ê°„)
    surge_items = detect_weekly_surge(token, org, project_id, environment, this_start_iso, this_end_iso, prev_start_iso, prev_end_iso)

    # ìµœì‹  ë¦´ë¦¬ì¦ˆì—ì„œ í•´ì†Œëœ ì´ìŠˆ(ì‚¬ë¼ì§„/ë§ì´ ê°ì†Œ)
    rfix = release_fixes_in_week(token, org, project_id, environment, this_start_iso, this_end_iso)

    payload = {
        "this_week_range_kst": this_range_label,
        "prev_week_range_kst": prev_range_label,
        "this_week": {
            "events": this_sum["events"],
            "issues": this_sum["issues"],
            "users":  this_sum["users"],
            "crash_free_sessions": cf_s,
            "crash_free_users":    cf_u,
        },
        "prev_week": prev_sum,
        "top5_events": top_events_this[:WEEKLY_TOP_LIMIT],
        "prev_top_events": top_events_prev[:WEEKLY_TOP_LIMIT],
        "new_issues": new_items,
        "surge_issues": surge_items,
        "this_week_release_fixes": rfix,
    }

    wlog(f"[12/{step_total}] ê²°ê³¼ JSON ë¯¸ë¦¬ë³´ê¸°:")
    print(json.dumps(payload, ensure_ascii=False, indent=2))

    if slack_webhook:
        title = f"Sentry ì£¼ê°„ ë¦¬í¬íŠ¸ â€” {this_range_label}"
        blocks = build_weekly_blocks(
            payload,
            title,
            environment,
            org,
            int(project_id),
            {"start": this_start_iso, "end": this_end_iso},
        )
        wlog(f"[13/{step_total}] Slack ì „ì†¡â€¦")
        post_to_slack(slack_webhook, blocks)
    else:
        wlog(f"[13/{step_total}] SLACK_WEBHOOK_URL ë¯¸ì„¤ì •: Slack ì „ì†¡ ìƒëµ.")

if __name__ == "__main__":
    main()