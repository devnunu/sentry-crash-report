"""
Sentry ì£¼ê°„ Android í¬ë˜ì‹œ ë¦¬í¬íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë§¤ì£¼ ì›”ìš”ì¼ì— ì§€ë‚œ 7ì¼ê°„ì˜ í¬ë˜ì‹œ í˜„í™©ì„ ë¶„ì„í•˜ì—¬ Slackìœ¼ë¡œ ì „ì†¡
"""

import json
import os
import re
import requests
import time
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, Tuple, List

# dotenv ì§€ì› (ë¡œì»¬ í™˜ê²½)
try:
    from dotenv import load_dotenv
    env_path = Path('.env')
    if env_path.exists():
        load_dotenv()
        print("âœ… .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    pass

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
SENTRY_TOKEN = os.getenv('SENTRY_AUTH_TOKEN')
ORG_SLUG = os.getenv('SENTRY_ORG_SLUG')
PROJECT_SLUG = os.getenv('SENTRY_PROJECT_SLUG')
PROJECT_ID = os.getenv('SENTRY_PROJECT_ID')
SLACK_WEBHOOK = os.getenv('SLACK_WEBHOOK_URL')
DASH_BOARD_ID = os.getenv('DASH_BOARD_ID')
ENVIRONMENT = os.getenv('SENTRY_ENVIRONMENT', 'Production')

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
TEST_MODE = os.getenv('TEST_MODE', 'false').lower() == 'true'

# ì¼ê´€ì„± ëª¨ë“œ (ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
CONSISTENCY_MODE = os.getenv('CONSISTENCY_MODE', 'false').lower() == 'true'

# í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ë•Œ ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
if TEST_MODE:
    DEBUG_DIR = Path('debug_output')
    DEBUG_DIR.mkdir(exist_ok=True)
    print("ğŸ§ª ì£¼ê°„ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”")

if CONSISTENCY_MODE:
    print("ğŸ¯ ì¼ê´€ì„± ëª¨ë“œ í™œì„±í™” - ë” ì •í™•í•œ ë°ì´í„°ë¥¼ ìœ„í•´ ì²˜ë¦¬ ì‹œê°„ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
if not all([SENTRY_TOKEN, ORG_SLUG, PROJECT_SLUG, PROJECT_ID]):
    print("âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    raise ValueError("Sentry ê´€ë ¨ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

try:
    PROJECT_ID = int(PROJECT_ID)
except (ValueError, TypeError):
    raise ValueError("SENTRY_PROJECT_IDëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")

# Sentry API ì„¤ì •
SENTRY_API_BASE = "https://sentry.io/api/0"
HEADERS = {
    'Authorization': f'Bearer {SENTRY_TOKEN}',
    'Content-Type': 'application/json'
}

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = timezone(timedelta(hours=9))


def get_weekly_datetime_range():
    """ì§€ë‚œ 7ì¼ê°„ ì‹œê°„ ë²”ìœ„ ê³„ì‚° (KST ê¸°ì¤€)"""
    target_date_str = os.getenv('TARGET_WEEK_START')

    if target_date_str:
        try:
            week_start = datetime.strptime(target_date_str, '%Y-%m-%d')
            week_start = week_start.replace(tzinfo=KST)
            print(f"ğŸ¯ ì§€ì •ëœ ì£¼ê°„ ì‹œì‘ì¼ ì‚¬ìš©: {target_date_str}")
        except ValueError:
            print(f"âš ï¸ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date_str}. ì§€ë‚œ 7ì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            now = datetime.now(KST)
            # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •
            today = now.replace(hour=0, minute=0, second=0, microsecond=0)
            week_start = today - timedelta(days=7)
    else:
        now = datetime.now(KST)
        # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê³ ì • (ì‹œê°„ ì œê±°)
        today = now.replace(hour=0, minute=0, second=0, microsecond=0)
        week_start = today - timedelta(days=7)
        print(f"ğŸ“… ê¸°ë³¸ ì£¼ê°„ ë²”ìœ„ ì‚¬ìš© (ì˜¤ëŠ˜ ê¸°ì¤€ ì§€ë‚œ 7ì¼)")

    # ì´ë²ˆ ì£¼: 7ì¼ ì „ 00:00 ~ ì–´ì œ 23:59 (1ë§ˆì´í¬ë¡œì´ˆ ë¹¼ì„œ ê²½ê³„ ëª…í™•íˆ)
    this_week_start = week_start
    this_week_end = (today - timedelta(days=1)).replace(hour=23, minute=59, second=59, microsecond=999999)

    # ì „ì£¼: 14ì¼ ì „ 00:00 ~ 8ì¼ ì „ 23:59
    prev_week_start = this_week_start - timedelta(days=7)
    prev_week_end = this_week_start - timedelta(microseconds=1)

    # UTCë¡œ ë³€í™˜
    this_week_start_utc = this_week_start.astimezone(timezone.utc)
    this_week_end_utc = this_week_end.astimezone(timezone.utc)
    prev_week_start_utc = prev_week_start.astimezone(timezone.utc)
    prev_week_end_utc = prev_week_end.astimezone(timezone.utc)

    return {
        'this_week': (this_week_start_utc, this_week_end_utc, this_week_start),
        'prev_week': (prev_week_start_utc, prev_week_end_utc, prev_week_start)
    }


def collect_issues_for_date(start_time: datetime, end_time: datetime) -> List[Dict]:
    """íŠ¹ì • ë‚ ì§œì˜ ëª¨ë“  ì´ìŠˆ ìˆ˜ì§‘"""

    start_str = start_time.isoformat()
    end_str = end_time.isoformat()

    issues_url = f"{SENTRY_API_BASE}/projects/{ORG_SLUG}/{PROJECT_SLUG}/issues/"
    all_issues = []
    all_issue_ids = set()  # ID ì¤‘ë³µ ì²´í¬ìš©

    # firstSeen ê¸°ì¤€ ì´ìŠˆ ìˆ˜ì§‘
    cursor = None
    page = 1

    while True:
        issues_params = {
            'query': f'firstSeen:>={start_str} firstSeen:<{end_str} environment:{ENVIRONMENT}',
            'limit': 100,
            'sort': 'date',
            'environment': ENVIRONMENT
        }

        if cursor:
            issues_params['cursor'] = cursor

        try:
            response = requests.get(issues_url, headers=HEADERS, params=issues_params)

            if response.status_code != 200:
                break

            page_issues = response.json()

            if not page_issues:
                break

            # IDê°€ ìœ íš¨í•œ ì´ìŠˆë§Œ ì¶”ê°€
            for issue in page_issues:
                issue_id = issue.get('id')
                if issue_id and issue_id not in all_issue_ids:
                    all_issues.append(issue)
                    all_issue_ids.add(issue_id)

            # ë‹¤ìŒ í˜ì´ì§€ ì²´í¬
            link_header = response.headers.get('Link', '')
            if 'rel="next"' not in link_header:
                break

            cursor_match = re.search(r'cursor=([^&>]+).*rel="next"', link_header)
            if cursor_match:
                cursor = cursor_match.group(1)
                page += 1
            else:
                break

        except Exception as e:
            if TEST_MODE:
                print(f"      âŒ firstSeen ì˜¤ë¥˜: {str(e)}")
            break

    # lastSeen ê¸°ì¤€ ì´ìŠˆ ì¶”ê°€ ìˆ˜ì§‘
    existing_issues_params = {
        'query': f'lastSeen:>={start_str} lastSeen:<{end_str} environment:{ENVIRONMENT}',
        'limit': 100,
        'sort': 'date',
        'environment': ENVIRONMENT
    }

    existing_cursor = None
    existing_page = 1

    while True:
        if existing_cursor:
            existing_issues_params['cursor'] = existing_cursor

        try:
            response = requests.get(issues_url, headers=HEADERS, params=existing_issues_params)

            if response.status_code != 200:
                break

            existing_issues = response.json()

            if not existing_issues:
                break

            # IDê°€ ìœ íš¨í•˜ê³  ì¤‘ë³µì´ ì•„ë‹Œ ì´ìŠˆë§Œ ì¶”ê°€
            new_count = 0
            for issue in existing_issues:
                issue_id = issue.get('id')
                if issue_id and issue_id not in all_issue_ids:
                    all_issues.append(issue)
                    all_issue_ids.add(issue_id)
                    new_count += 1

            # ë‹¤ìŒ í˜ì´ì§€ ì²´í¬
            link_header = response.headers.get('Link', '')
            if 'rel="next"' not in link_header:
                break

            cursor_match = re.search(r'cursor=([^&>]+).*rel="next"', link_header)
            if cursor_match:
                existing_cursor = cursor_match.group(1)
                existing_page += 1
            else:
                break

        except Exception as e:
            if TEST_MODE:
                print(f"      âŒ lastSeen ì˜¤ë¥˜: {str(e)}")
            break

    return all_issues


def calculate_crash_stats_for_date(all_issues: List[Dict], start_time: datetime, end_time: datetime) -> int:
    """íŠ¹ì • ë‚ ì§œì˜ í¬ë˜ì‹œ í†µê³„ ê³„ì‚° (í¬ë˜ì‹œ ì´ìŠˆë§Œ í•„í„°ë§)"""

    # í¬ë˜ì‹œ ì´ìŠˆë§Œ í•„í„°ë§ (error, fatalë§Œ)
    crash_issues = []
    non_crash_levels = set()  # ë””ë²„ê¹…ìš©

    for issue in all_issues:
        level = issue.get('level', '').lower()
        if level in ['error', 'fatal']:
            # IDê°€ ìˆëŠ” ì´ìŠˆë§Œ ì²˜ë¦¬
            if issue.get('id'):
                crash_issues.append(issue)
        else:
            non_crash_levels.add(level)

    if TEST_MODE:
        day_str = start_time.astimezone(KST).strftime('%m/%d')
        print(f"         ğŸ“Š {day_str}: ì´ {len(all_issues)}ê°œ ì´ìŠˆ ì¤‘ {len(crash_issues)}ê°œ í¬ë˜ì‹œ ì´ìŠˆ")
        if non_crash_levels:
            print(f"         ğŸ” í¬ë˜ì‹œê°€ ì•„ë‹Œ ë ˆë²¨ë“¤: {non_crash_levels}")

    # ì„±ëŠ¥ ìµœì í™”: ì´ìŠˆê°€ ë§ìœ¼ë©´ ì œí•œ
    if len(crash_issues) > 150:  # 100ì—ì„œ 150ìœ¼ë¡œ ì¦ê°€
        crash_issues_sorted = sorted(crash_issues, key=lambda x: safe_int(x.get('count', 0)), reverse=True)
        crash_issues = crash_issues_sorted[:150]
        if TEST_MODE:
            print(f"         âš¡ ìƒìœ„ 150ê°œ ì´ìŠˆë§Œ ì²˜ë¦¬")

    # í¬ë˜ì‹œ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚°
    total_events = 0
    crash_issues_with_events = []

    for i, issue in enumerate(crash_issues):
        issue_id = issue.get('id')
        if not issue_id:
            continue

        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if (i + 1) % 10 == 0 or i == 0:
            day_str = start_time.astimezone(KST).strftime('%m/%d')
            print(f"         ğŸ”„ {day_str}: {i + 1}/{len(crash_issues)} í¬ë˜ì‹œ ì´ìŠˆ ì²˜ë¦¬ ì¤‘...")

        # ì´ë²¤íŠ¸ ìˆ˜ ì¡°íšŒ
        event_count = get_issue_events_count_optimized(issue, issue_id, start_time, end_time)

        if event_count > 0:
            issue['event_count'] = event_count
            total_events += event_count
            crash_issues_with_events.append(issue)

            # TEST ëª¨ë“œì—ì„œ ìƒì„¸ ì •ë³´ ì¶œë ¥
            if TEST_MODE and len(crash_issues_with_events) <= 3:
                print(f"            âœ… í¬ë˜ì‹œ: {issue.get('title', '')[:40]}")
                print(f"               - ì´ë²¤íŠ¸: {event_count}ê±´")
                print(f"               - ë ˆë²¨: {issue.get('level', 'unknown')}")

        # API í˜¸ì¶œ ê°„ ë”œë ˆì´ (rate limit ë°©ì§€)
        if (i + 1) % 10 == 0:
            time.sleep(0.1)

    return total_events


def get_issue_events_count_optimized(issue: Dict, issue_id: str, start_time: datetime, end_time: datetime) -> int:
    """ìµœì í™”ëœ ì´ìŠˆ ì´ë²¤íŠ¸ ìˆ˜ ì¡°íšŒ"""

    # ìµœì í™” 1: ì´ìŠˆì˜ stats ë°ì´í„° ë¨¼ì € í™•ì¸
    try:
        # ë¨¼ì € ì´ìŠˆì˜ lastSeen í™•ì¸
        last_seen_str = issue.get('lastSeen')
        if last_seen_str:
            try:
                last_seen = datetime.fromisoformat(last_seen_str.replace('Z', '+00:00'))
                # lastSeenì´ ì‹œì‘ ì‹œê°„ë³´ë‹¤ ì´ì „ì´ë©´ í•´ë‹¹ ê¸°ê°„ì— ì´ë²¤íŠ¸ ì—†ìŒ
                if last_seen < start_time:
                    return 0
            except:
                pass

        # stats ë°ì´í„° í™•ì¸ (24h ë°ì´í„°ëŠ” ì‹ ë¢°ì„±ì´ ë‚®ìœ¼ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        # ë°”ë¡œ ì‹¤ì œ ì´ë²¤íŠ¸ ì¡°íšŒë¡œ ì´ë™

    except Exception:
        pass

    # ì§ì ‘ ì´ë²¤íŠ¸ ì¡°íšŒ
    return get_issue_events_count_for_date_limited(issue_id, start_time, end_time)


def get_issue_events_count_for_date_limited(issue_id: str, start_time: datetime, end_time: datetime) -> int:
    """ì œí•œì  ì´ë²¤íŠ¸ ìˆ˜ ì¡°íšŒ"""

    events_url = f"{SENTRY_API_BASE}/issues/{issue_id}/events/"
    all_events = []
    max_pages = 5 if CONSISTENCY_MODE else 3  # ì¼ê´€ì„± ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ í˜ì´ì§€ ì¡°íšŒ
    page = 0
    cursor = None

    while page < max_pages:
        params = {
            'limit': 100
        }

        if cursor:
            params['cursor'] = cursor

        try:
            response = requests.get(events_url, headers=HEADERS, params=params, timeout=15 if CONSISTENCY_MODE else 10)

            if response.status_code != 200:
                break

            events = response.json()

            if not events:
                break

            # ì‹œê°„ ë²”ìœ„ ë‚´ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
            found_in_range = False
            for event in events:
                event_time_str = event.get('dateCreated')
                if event_time_str:
                    try:
                        event_time = datetime.fromisoformat(event_time_str.replace('Z', '+00:00'))
                        if start_time <= event_time <= end_time:
                            all_events.append(event)
                            found_in_range = True
                        elif event_time < start_time:
                            return len(all_events)
                    except:
                        pass

            # í•´ë‹¹ ë²”ìœ„ì— ì´ë²¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì¡°ê¸° ì¤‘ë‹¨
            if not found_in_range and page > 0:
                break

            # ë‹¤ìŒ í˜ì´ì§€ ì²´í¬
            link_header = response.headers.get('Link', '')
            if 'rel="next"' not in link_header:
                break

            cursor_match = re.search(r'cursor=([^&>]+).*rel="next"', link_header)
            if cursor_match:
                cursor = cursor_match.group(1)
                page += 1
            else:
                break

        except requests.exceptions.Timeout:
            break
        except Exception:
            break

    return len(all_events)


def collect_daily_crash_data_simple(week_start_utc: datetime, week_label: str = "ì´ë²ˆì£¼") -> List[int]:
    """ì¼ê°„ ë¦¬í¬íŠ¸ ë¡œì§ì„ ì‚¬ìš©í•œ ì •í™•í•œ ì¼ë³„ í¬ë˜ì‹œ ë°ì´í„° ìˆ˜ì§‘"""
    daily_crashes = []
    days = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']

    print(f"   ğŸ“Š {week_label} 7ì¼ê°„ ì¼ë³„ í¬ë˜ì‹œ ë¶„ì„ ì‹œì‘...")

    # ê¸°ì¤€ ì‹œê°„ì„ KSTë¡œ ë³€í™˜
    week_start_kst = week_start_utc.astimezone(KST)
    print(f"   ğŸ• ê¸°ì¤€ ì‹œì‘ ì‹œê°„ (KST): {week_start_kst.strftime('%Y-%m-%d %H:%M:%S')}")

    # ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ì„ ìœ„í•œ ë¡œê·¸
    print(f"   ğŸ” ì‹¤í–‰ ì‹œê°: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')} KST")

    # 7ì¼ê°„ ê°ê° ì¼ê°„ ë¦¬í¬íŠ¸ ë¡œì§ ì ìš©
    for day in range(7):
        # KST ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        day_kst_start = week_start_kst + timedelta(days=day)
        day_kst_start = day_kst_start.replace(hour=0, minute=0, second=0, microsecond=0)
        day_kst_end = day_kst_start.replace(hour=23, minute=59, second=59, microsecond=999999)

        # KSTë¥¼ UTCë¡œ ë³€í™˜
        day_start_utc = day_kst_start.astimezone(timezone.utc)
        day_end_utc = day_kst_end.astimezone(timezone.utc)

        day_name = days[day]

        # ìƒì„¸í•œ ì‹œê°„ ë²”ìœ„ ì¶œë ¥
        print(f"   ğŸ”„ [{day+1}/7] {day_name}ìš”ì¼ ë¶„ì„:")
        print(f"      ğŸ“… KST: {day_kst_start.strftime('%Y-%m-%d %H:%M:%S')} ~ {day_kst_end.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"      ğŸ“… UTC: {day_start_utc.strftime('%Y-%m-%d %H:%M:%S')} ~ {day_end_utc.strftime('%Y-%m-%d %H:%M:%S')}")

        # ì¼ê°„ ë¦¬í¬íŠ¸ì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ í•´ë‹¹ ë‚ ì§œ ì´ìŠˆ ìˆ˜ì§‘
        day_issues = collect_issues_for_date(day_start_utc, day_end_utc)

        # í¬ë˜ì‹œ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚°
        day_crashes = calculate_crash_stats_for_date(day_issues, day_start_utc, day_end_utc)

        daily_crashes.append(day_crashes)

        print(f"      âœ… {day_name}ìš”ì¼ ({day_kst_start.strftime('%m/%d')}): {day_crashes}ê±´")
        print()

        # API í˜¸ì¶œ ê°„ ë”œë ˆì´
        time.sleep(0.3)

    print(f"   ğŸ“ˆ {week_label} ì¼ë³„ ë¶„ì„ ì™„ë£Œ: {daily_crashes}")
    print(f"   ğŸ“ˆ {week_label} ì´í•©: {sum(daily_crashes)}ê±´")

    # ìºì‹œ í‚¤ ìƒì„± (ë””ë²„ê·¸ìš©)
    cache_key = f"{week_start_kst.strftime('%Y%m%d')}-{(week_start_kst + timedelta(days=6)).strftime('%Y%m%d')}"
    print(f"   ğŸ”‘ ìºì‹œ í‚¤: {cache_key}")

    return daily_crashes


def collect_weekly_issues(start_time: datetime, end_time: datetime, week_label: str) -> List[Dict]:
    """ì£¼ê°„ ì´ìŠˆ ìˆ˜ì§‘"""
    issues_url = f"{SENTRY_API_BASE}/projects/{ORG_SLUG}/{PROJECT_SLUG}/issues/"
    all_issues = []

    start_str = start_time.isoformat()
    end_str = end_time.isoformat()

    # firstSeen ê¸°ì¤€ ì´ìŠˆ ìˆ˜ì§‘
    cursor = None
    page = 1

    while True:
        issues_params = {
            'query': f'firstSeen:>={start_str} firstSeen:<{end_str} environment:{ENVIRONMENT}',
            'limit': 100,
            'sort': 'date',
            'environment': ENVIRONMENT
        }

        if cursor:
            issues_params['cursor'] = cursor

        if TEST_MODE:
            print(f"ğŸ” {week_label} firstSeen í˜ì´ì§€ {page} ì¡°íšŒ...")

        try:
            response = requests.get(issues_url, headers=HEADERS, params=issues_params)
            if response.status_code != 200:
                break

            page_issues = response.json()
            if not page_issues:
                break

            all_issues.extend(page_issues)

            if TEST_MODE:
                print(f"   í˜ì´ì§€ {page}: {len(page_issues)}ê°œ ìˆ˜ì§‘ (ì´ {len(all_issues)}ê°œ)")

            # ë‹¤ìŒ í˜ì´ì§€ ì²´í¬
            link_header = response.headers.get('Link', '')
            if 'rel="next"' not in link_header:
                break

            cursor_match = re.search(r'cursor=([^&>]+).*rel="next"', link_header)
            if cursor_match:
                cursor = cursor_match.group(1)
                page += 1
            else:
                break

        except Exception as e:
            if TEST_MODE:
                print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
            break

    # lastSeen ê¸°ì¤€ ì´ìŠˆ ì¶”ê°€ ìˆ˜ì§‘
    existing_issues_params = {
        'query': f'lastSeen:>={start_str} lastSeen:<{end_str} environment:{ENVIRONMENT}',
        'limit': 100,
        'sort': 'date',
        'environment': ENVIRONMENT
    }

    existing_cursor = None
    existing_page = 1

    while True:
        if existing_cursor:
            existing_issues_params['cursor'] = existing_cursor

        try:
            response = requests.get(issues_url, headers=HEADERS, params=existing_issues_params)
            if response.status_code != 200:
                break

            existing_issues = response.json()
            if not existing_issues:
                break

            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
            existing_issue_ids = {issue.get('id') for issue in all_issues}
            new_count = 0
            for issue in existing_issues:
                if issue.get('id') not in existing_issue_ids:
                    all_issues.append(issue)
                    new_count += 1

            if TEST_MODE and existing_page <= 2:
                print(f"   {week_label} lastSeen í˜ì´ì§€ {existing_page}: {new_count}ê°œ ìƒˆë¡œ ì¶”ê°€")

            # ë‹¤ìŒ í˜ì´ì§€ ì²´í¬
            link_header = response.headers.get('Link', '')
            if 'rel="next"' not in link_header:
                break

            cursor_match = re.search(r'cursor=([^&>]+).*rel="next"', link_header)
            if cursor_match:
                existing_cursor = cursor_match.group(1)
                existing_page += 1
            else:
                break

        except Exception as e:
            if TEST_MODE:
                print(f"   âŒ {week_label} lastSeen ì˜¤ë¥˜: {str(e)}")
            break

    return all_issues


def analyze_issue_lifecycle_improved(this_week_issues: List[Dict], prev_week_issues: List[Dict],
                                   this_week_start: datetime, this_week_end: datetime,
                                   prev_week_start: datetime, prev_week_end: datetime) -> Dict:
    """ê°œì„ ëœ ì´ìŠˆ ìƒëª…ì£¼ê¸° ë¶„ì„ (ì •í™•í•œ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚°)"""
    # í¬ë˜ì‹œ ì´ìŠˆë§Œ í•„í„°ë§
    this_week_crash_issues = []
    prev_week_crash_issues = []

    for issue in this_week_issues:
        level = issue.get('level', '').lower()
        if level in ['error', 'fatal']:
            this_week_crash_issues.append(issue)

    for issue in prev_week_issues:
        level = issue.get('level', '').lower()
        if level in ['error', 'fatal']:
            prev_week_crash_issues.append(issue)

    print(f"   ğŸ“Š í¬ë˜ì‹œ ì´ìŠˆ í•„í„°ë§: ì´ë²ˆì£¼ {len(this_week_crash_issues)}ê°œ, ì „ì£¼ {len(prev_week_crash_issues)}ê°œ")

    # ì´ìŠˆë¥¼ IDë¡œ ë§¤í•‘
    this_week_map = {issue['id']: issue for issue in this_week_crash_issues}
    prev_week_map = {issue['id']: issue for issue in prev_week_crash_issues}

    this_week_ids = set(this_week_map.keys())
    prev_week_ids = set(prev_week_map.keys())

    # ê° ì´ìŠˆì˜ ì£¼ê°„ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚° (ìƒìœ„ 30ê°œë§Œ ì •í™•íˆ ê³„ì‚°)
    print(f"   ğŸ”„ ì´ë²ˆì£¼ ìƒìœ„ ì´ìŠˆì˜ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚° ì¤‘...")

    # ì´ë²ˆ ì£¼ ì´ìŠˆë“¤ì˜ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚°
    for i, issue in enumerate(sorted(this_week_crash_issues,
                                   key=lambda x: safe_int(x.get('count', 0)),
                                   reverse=True)[:30]):
        issue_id = issue.get('id')
        if issue_id:
            event_count = get_issue_events_count_for_week(issue_id, this_week_start, this_week_end)
            issue['this_week_events'] = event_count

            if (i + 1) % 10 == 0:
                print(f"      {i + 1}/30 ì™„ë£Œ...")

    # ì „ì£¼ ì´ìŠˆë“¤ì˜ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚° (ì´ë²ˆì£¼ì™€ ê²¹ì¹˜ëŠ” ì´ìŠˆë§Œ)
    print(f"   ğŸ”„ ì „ì£¼ ì´ìŠˆì˜ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚° ì¤‘...")
    common_issue_ids = this_week_ids & prev_week_ids

    for i, issue_id in enumerate(common_issue_ids):
        if issue_id in prev_week_map:
            prev_issue = prev_week_map[issue_id]
            event_count = get_issue_events_count_for_week(issue_id, prev_week_start, prev_week_end)
            prev_issue['prev_week_events'] = event_count

            # ì´ë²ˆ ì£¼ ì´ìŠˆì—ë„ ì „ì£¼ ì´ë²¤íŠ¸ ìˆ˜ ì €ì¥
            if issue_id in this_week_map:
                this_week_map[issue_id]['prev_week_events'] = event_count

    # 1. ì§„ì§œ ì‹ ê·œ ì´ìŠˆ (firstSeenì´ ì´ë²ˆ ì£¼ ë²”ìœ„ ë‚´)
    new_issues = []
    for issue_id in this_week_ids:
        issue = this_week_map[issue_id]
        first_seen_str = issue.get('firstSeen')

        if first_seen_str:
            try:
                first_seen = datetime.fromisoformat(first_seen_str.replace('Z', '+00:00'))
                # ì´ë²ˆ ì£¼ì— ì²˜ìŒ ë°œìƒí•œ ì´ìŠˆë§Œ ì‹ ê·œë¡œ ë¶„ë¥˜
                if first_seen >= this_week_start:
                    count = issue.get('this_week_events', safe_int(issue.get('count', 0)))
                    if count > 0:
                        new_issues.append({
                            'issue': issue,
                            'count': count,
                            'first_seen': first_seen
                        })
            except:
                pass

    new_issues.sort(key=lambda x: x['count'], reverse=True)

    # 2. ì•…í™”ëœ ì´ìŠˆ (ì „ì£¼ ëŒ€ë¹„ 50% ì´ìƒ ì¦ê°€)
    worsened_issues = []
    for issue_id in common_issue_ids:
        issue = this_week_map[issue_id]
        this_count = issue.get('this_week_events', 0)
        prev_count = issue.get('prev_week_events', 0)

        if prev_count > 0 and this_count > prev_count * 1.5:  # 50% ì´ìƒ ì¦ê°€
            increase_rate = ((this_count - prev_count) / prev_count) * 100
            worsened_issues.append({
                'issue': issue,
                'this_count': this_count,
                'prev_count': prev_count,
                'increase_rate': increase_rate
            })

    worsened_issues.sort(key=lambda x: x['increase_rate'], reverse=True)

    # 3. ê°œì„ ëœ ì´ìŠˆ (ì „ì£¼ ëŒ€ë¹„ 50% ì´ìƒ ê°ì†Œ)
    improved_issues = []
    for issue_id in common_issue_ids:
        issue = this_week_map[issue_id]
        this_count = issue.get('this_week_events', 0)
        prev_count = issue.get('prev_week_events', 0)

        if prev_count > 0 and this_count < prev_count * 0.5:  # 50% ì´ìƒ ê°ì†Œ
            decrease_rate = ((prev_count - this_count) / prev_count) * 100
            improved_issues.append({
                'issue': issue,
                'this_count': this_count,
                'prev_count': prev_count,
                'decrease_rate': decrease_rate
            })

    improved_issues.sort(key=lambda x: x['decrease_rate'], reverse=True)

    # 4. í•´ê²°ëœ ì´ìŠˆ (ì „ì£¼ì—ëŠ” ìˆì—ˆì§€ë§Œ ì´ë²ˆì£¼ì—ëŠ” ì—†ìŒ)
    resolved_issues = []
    for issue_id in prev_week_ids - this_week_ids:
        prev_issue = prev_week_map[issue_id]
        # ì „ì£¼ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚°
        prev_count = get_issue_events_count_for_week(issue_id, prev_week_start, prev_week_end)

        if prev_count >= 10:  # ì „ì£¼ì— 10ê±´ ì´ìƒì´ì—ˆë˜ ì´ìŠˆë§Œ
            resolved_issues.append({
                'issue': prev_issue,
                'prev_count': prev_count
            })

    resolved_issues.sort(key=lambda x: x['prev_count'], reverse=True)

    print(f"   âœ… ìƒëª…ì£¼ê¸° ë¶„ì„ ì™„ë£Œ:")
    print(f"      - ì‹ ê·œ: {len(new_issues)}ê°œ")
    print(f"      - ì•…í™”: {len(worsened_issues)}ê°œ")
    print(f"      - ê°œì„ : {len(improved_issues)}ê°œ")
    print(f"      - í•´ê²°: {len(resolved_issues)}ê°œ")

    return {
        'new': new_issues[:5],
        'worsened': worsened_issues[:5],
        'improved': improved_issues[:5],
        'resolved': resolved_issues[:5]
    }


def get_issue_events_count_for_week(issue_id: str, start_time: datetime, end_time: datetime) -> int:
    """íŠ¹ì • ì´ìŠˆì˜ ì£¼ê°„ ì´ë²¤íŠ¸ ìˆ˜ ì¡°íšŒ (ê°„ì†Œí™”)"""
    events_url = f"{SENTRY_API_BASE}/issues/{issue_id}/events/"
    total_events = 0
    cursor = None
    max_pages = 5  # ìµœëŒ€ 5í˜ì´ì§€

    for page in range(max_pages):
        params = {'limit': 100}
        if cursor:
            params['cursor'] = cursor

        try:
            response = requests.get(events_url, headers=HEADERS, params=params, timeout=10)

            if response.status_code != 200:
                break

            events = response.json()
            if not events:
                break

            # ì‹œê°„ ë²”ìœ„ ë‚´ ì´ë²¤íŠ¸ë§Œ ì¹´ìš´íŠ¸
            for event in events:
                event_time_str = event.get('dateCreated')
                if event_time_str:
                    try:
                        event_time = datetime.fromisoformat(event_time_str.replace('Z', '+00:00'))
                        if start_time <= event_time <= end_time:
                            total_events += 1
                        elif event_time < start_time:
                            return total_events
                    except:
                        pass

            # ë‹¤ìŒ í˜ì´ì§€ ì²´í¬
            link_header = response.headers.get('Link', '')
            if 'rel="next"' not in link_header:
                break

            cursor_match = re.search(r'cursor=([^&>]+).*rel="next"', link_header)
            if cursor_match:
                cursor = cursor_match.group(1)
            else:
                break

        except:
            break

    return total_events


def detect_anomalies_simple(this_week_daily: List[int]) -> List[str]:
    """ê°„ë‹¨í•œ ì´ìƒ ì§•í›„ íƒì§€"""
    anomalies = []

    if not this_week_daily or len(this_week_daily) < 7:
        return anomalies

    avg_crashes = sum(this_week_daily) / len(this_week_daily)
    days = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']

    for i, crashes in enumerate(this_week_daily):
        day_name = days[i]

        # ê¸‰ì¦ ê°ì§€ (í‰ê·  ëŒ€ë¹„ 100% ì´ìƒ)
        if avg_crashes > 0 and crashes > avg_crashes * 2:
            anomalies.append(f"{day_name}ìš”ì¼ ê¸‰ì¦: {crashes}ê±´ (ì´ë²ˆì£¼ í‰ê·  {avg_crashes:.0f}ê±´ ëŒ€ë¹„ +{((crashes/avg_crashes-1)*100):.0f}%)")

        # ì„ê³„ì  ëŒíŒŒ (ì¼ 100ê±´ ì´ˆê³¼)
        if crashes > 100:
            anomalies.append(f"{day_name}ìš”ì¼ ì„ê³„ì  ëŒíŒŒ: {crashes}ê±´ (ê¸°ì¤€: 100ê±´)")

    # ì—°ì† ì¦ê°€ íŒ¨í„´ ê°ì§€
    consecutive_increases = 0
    for i in range(1, len(this_week_daily)):
        if this_week_daily[i] > this_week_daily[i-1]:
            consecutive_increases += 1
        else:
            if consecutive_increases >= 2:  # 3ì¼ ì´ìƒ ì—°ì† ì¦ê°€
                end_day = days[i-1]
                start_day = days[i-consecutive_increases-1]
                start_count = this_week_daily[i-consecutive_increases-1]
                end_count = this_week_daily[i-1]
                anomalies.append(f"{start_day}-{end_day} ì—°ì† ì¦ê°€: {start_count}ê±´ â†’ {end_count}ê±´ ({consecutive_increases+1}ì¼ê°„)")
            consecutive_increases = 0

    # ë§ˆì§€ë§‰ ì²´í¬
    if consecutive_increases >= 2:
        end_day = days[-1]
        start_day = days[-consecutive_increases-2]
        start_count = this_week_daily[-consecutive_increases-2]
        end_count = this_week_daily[-1]
        anomalies.append(f"{start_day}-{end_day} ì—°ì† ì¦ê°€: {start_count}ê±´ â†’ {end_count}ê±´ ({consecutive_increases+1}ì¼ê°„)")

    return anomalies


def get_weekly_crash_free_rate():
    """ì£¼ê°„ Crash-Free Rate ì¡°íšŒ"""
    sessions_url = f"{SENTRY_API_BASE}/organizations/{ORG_SLUG}/sessions/"

    end_time = datetime.now(timezone.utc)
    start_time = end_time - timedelta(days=7)

    params = {
        'field': ['crash_free_rate(session)'],
        'start': start_time.isoformat(),
        'end': end_time.isoformat(),
        'project': [PROJECT_ID],
        'environment': [ENVIRONMENT],
        'totals': 1
    }

    try:
        response = requests.get(sessions_url, headers=HEADERS, params=params, timeout=30)

        if response.status_code == 200:
            data = response.json()

            if 'groups' in data and data['groups']:
                for group in data['groups']:
                    totals = group.get('totals', {})
                    session_crash_free = totals.get('crash_free_rate(session)')

                    if session_crash_free is not None:
                        rate = session_crash_free * 100 if session_crash_free <= 1 else session_crash_free
                        return f"{rate:.2f}%"

    except Exception as e:
        if TEST_MODE:
            print(f"   âŒ ì£¼ê°„ Crash-Free Rate ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

    return "N/A"


def safe_int(value, default=0):
    """ì•ˆì „í•œ ì •ìˆ˜ ë³€í™˜"""
    try:
        if isinstance(value, (int, float)):
            return int(value)
        elif isinstance(value, str) and value.isdigit():
            return int(value)
        else:
            return default
    except (ValueError, TypeError):
        return default


def format_issue_title(title: str, max_length: int = 40) -> str:
    """ì´ìŠˆ ì œëª© í¬ë§·íŒ…"""
    if len(title) > max_length:
        title = title[:max_length - 3] + "..."

    # Slack íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
    title = title.replace('*', '').replace('_', '').replace('`', '')
    return title


def format_weekly_slack_message(this_week_total: int, prev_week_total: int,
                                this_week_stats: Dict, lifecycle: Dict,
                                anomalies: List[str], crash_free_rate: str,
                                week_info: Dict, this_week_daily: List[int]) -> Dict:
    """ì£¼ê°„ Slack ë©”ì‹œì§€ í¬ë§·íŒ… (ìš”ì¼ë³„ í•©ê³„ ê¸°ì¤€)"""

    this_week_start_kst = week_info['this_week'][2]
    this_week_end_kst = this_week_start_kst + timedelta(days=6)

    week_range = f"{this_week_start_kst.strftime('%Yë…„ %mì›” %dì¼')} ~ {this_week_end_kst.strftime('%mì›” %dì¼')}"

    # ì „ì£¼ ëŒ€ë¹„ ë³€í™” ê³„ì‚° (ìš”ì¼ë³„ í•©ê³„ ê¸°ì¤€)
    current = this_week_total
    previous = prev_week_total

    change_text = ""
    if previous == 0 and current == 0:
        change_text = " (ë³€í™” ì—†ìŒ â¡ï¸)"
        status_color = "good"
        main_emoji = "âœ¨"
        status_text = "ì•ˆì •ì "
    elif previous == 0:
        change_text = " (ì‹ ê·œ ë°œìƒ ğŸš¨)"
        status_color = "danger"
        main_emoji = "ğŸš¨"
        status_text = "ì£¼ì˜ í•„ìš”"
    elif current == 0:
        change_text = " (ì™„ì „ í•´ê²° ğŸ‰)"
        status_color = "good"
        main_emoji = "ğŸ‰"
        status_text = "ì™„ë²½!"
    else:
        change_count = current - previous
        if change_count > 0:
            change_text = f" (ì „ì£¼ ëŒ€ë¹„ +{change_count}ê±´ ğŸ“ˆ)"
            status_color = "warning" if change_count < 100 else "danger"
            main_emoji = "âš ï¸" if change_count < 100 else "ğŸš¨"
            status_text = "ì¦ê°€" if change_count < 100 else "ê¸‰ì¦"
        elif change_count < 0:
            change_text = f" (ì „ì£¼ ëŒ€ë¹„ {change_count}ê±´ ğŸ“‰)"
            status_color = "good"
            main_emoji = "âœ…"
            status_text = "ê°œì„ "
        else:
            change_text = " (ì „ì£¼ì™€ ë™ì¼ â¡ï¸)"
            status_color = "good"
            main_emoji = "â¡ï¸"
            status_text = "ì•ˆì •ì "

    # ì¼í‰ê·  ê³„ì‚°
    daily_avg = current // 7 if current > 0 else 0

    # ìš”ì¼ë³„ í¬ë˜ì‹œ í˜„í™© í…ìŠ¤íŠ¸ ìƒì„±
    days = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
    daily_text = ""

    for i, (day, count) in enumerate(zip(days, this_week_daily)):
        daily_text += f"{day} {count}ê±´ "

    test_indicator = " [í…ŒìŠ¤íŠ¸]" if TEST_MODE else ""

    # ê¸°ë³¸ ë¸”ë¡ë“¤
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"Android ì£¼ê°„ í¬ë˜ì‹œ ë¦¬í¬íŠ¸{test_indicator}",
                "emoji": True
            }
        },
        {
            "type": "context",
            "elements": [
                {
                    "type": "mrkdwn",
                    "text": f"ğŸ“… {week_range} | ğŸŒ {ENVIRONMENT} | ìƒíƒœ: {main_emoji} {status_text}"
                }
            ]
        },
        {
            "type": "divider"
        },
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ğŸ“Š ì£¼ìš” ì§€í‘œ*"
            }
        },
        {
            "type": "section",
            "fields": [
                {
                    "type": "mrkdwn",
                    "text": f"*ì£¼ê°„ ì´ í¬ë˜ì‹œ*\n{current:,}ê±´ (ì¼í‰ê·  {daily_avg}ê±´){change_text}"
                },
                {
                    "type": "mrkdwn",
                    "text": f"*ì˜í–¥ë°›ì€ ì‚¬ìš©ì*\n{this_week_stats['affected_users']:,}ëª…"
                },
                {
                    "type": "mrkdwn",
                    "text": f"*í¬ë˜ì‹œ ì´ìŠˆ ì¢…ë¥˜*\n{this_week_stats['total_issues']}ê°œ"
                },
                {
                    "type": "mrkdwn",
                    "text": f"*ì£¼ê°„ Crash-Free Rate*\n{crash_free_rate}"
                }
            ]
        },
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ğŸ“ˆ ìš”ì¼ë³„ í¬ë˜ì‹œ í˜„í™©*\n{daily_text.strip()}"
            }
        }
    ]

    # ì´ìƒ ì§•í›„ ì„¹ì…˜ (ì¡°ê±´ë¶€)
    if anomalies:
        blocks.extend([
            {
                "type": "divider"
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*âš ï¸ ì´ë²ˆ ì£¼ ì´ìƒ ì§•í›„ ê°ì§€*\n" + "\n".join([f"â€¢ {anomaly}" for anomaly in anomalies[:3]])
                }
            }
        ])

    # ì´ìŠˆ ìƒëª…ì£¼ê¸° ì„¹ì…˜
    lifecycle_text = ""

    if lifecycle['new']:
        lifecycle_text += f"ğŸ†• *ì‹ ê·œ ë°œìƒ ({len(lifecycle['new'])}ê°œ)*\n"
        for i, item in enumerate(lifecycle['new'][:3], 1):
            issue = item['issue']
            issue_id = issue.get('id', '')
            title = format_issue_title(issue.get('title', 'Unknown'))
            count = item['count']
            first_seen = item.get('first_seen')

            # Sentry ì´ìŠˆ ë§í¬ ìƒì„±
            issue_link = f"https://sentry.io/organizations/{ORG_SLUG}/issues/{issue_id}/"

            if first_seen:
                first_seen_kst = first_seen.astimezone(KST)
                date_str = first_seen_kst.strftime('%m/%d')
                lifecycle_text += f"  {i}. <{issue_link}|{title}> - {count}ê±´ ({date_str} ì²« ë°œìƒ)\n"
            else:
                lifecycle_text += f"  {i}. <{issue_link}|{title}> - {count}ê±´\n"
        lifecycle_text += "\n"

    if lifecycle['worsened']:
        lifecycle_text += f"âš ï¸ *ì•…í™” ({len(lifecycle['worsened'])}ê°œ)*\n"
        for i, item in enumerate(lifecycle['worsened'][:2], 1):
            issue = item['issue']
            issue_id = issue.get('id', '')
            title = format_issue_title(issue.get('title', 'Unknown'))
            rate = item['increase_rate']

            # Sentry ì´ìŠˆ ë§í¬ ìƒì„±
            issue_link = f"https://sentry.io/organizations/{ORG_SLUG}/issues/{issue_id}/"

            lifecycle_text += f"  {i}. <{issue_link}|{title}> +{rate:.0f}% ({item['prev_count']}â†’{item['this_count']}ê±´)\n"
        lifecycle_text += "\n"

    if lifecycle['improved']:
        lifecycle_text += f"âœ… *ê°œì„  ({len(lifecycle['improved'])}ê°œ)*\n"
        for i, item in enumerate(lifecycle['improved'][:2], 1):
            issue = item['issue']
            issue_id = issue.get('id', '')
            title = format_issue_title(issue.get('title', 'Unknown'))
            rate = item['decrease_rate']

            # Sentry ì´ìŠˆ ë§í¬ ìƒì„±
            issue_link = f"https://sentry.io/organizations/{ORG_SLUG}/issues/{issue_id}/"

            lifecycle_text += f"  {i}. <{issue_link}|{title}> -{rate:.0f}% ({item['prev_count']}â†’{item['this_count']}ê±´)\n"
        lifecycle_text += "\n"

    if lifecycle['resolved']:
        lifecycle_text += f"ğŸ‰ *í•´ê²° ì™„ë£Œ ({len(lifecycle['resolved'])}ê°œ)*\n"
        for i, item in enumerate(lifecycle['resolved'][:2], 1):
            issue = item['issue']
            issue_id = issue.get('id', '')
            title = format_issue_title(issue.get('title', 'Unknown'))
            prev_count = item['prev_count']

            # Sentry ì´ìŠˆ ë§í¬ ìƒì„±
            issue_link = f"https://sentry.io/organizations/{ORG_SLUG}/issues/{issue_id}/"

            lifecycle_text += f"  {i}. <{issue_link}|{title}> (ì „ì£¼ {prev_count}ê±´ â†’ í•´ê²°)\n"

    if not lifecycle_text:
        lifecycle_text = "ì´ë²ˆ ì£¼ëŠ” íŠ¹ë³„í•œ ë³€í™”ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤."

    blocks.extend([
        {
            "type": "divider"
        },
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ğŸ”„ ì´ìŠˆ ìƒëª…ì£¼ê¸° ë¶„ì„*\n{lifecycle_text}"
            }
        }
    ])

    # ëŒ€ì‹œë³´ë“œ ë§í¬
    dashboard_url = f"https://finda-b2c.sentry.io/dashboard/{DASH_BOARD_ID}" if DASH_BOARD_ID else "https://finda-b2c.sentry.io/dashboards"
    button_text = "Sentry ëŒ€ì‹œë³´ë“œ ì—´ê¸°" if DASH_BOARD_ID else "Sentry ëŒ€ì‹œë³´ë“œ ëª©ë¡ ì—´ê¸°"

    blocks.extend([
        {
            "type": "actions",
            "elements": [
                {
                    "type": "button",
                    "text": {
                        "type": "plain_text",
                        "text": button_text,
                        "emoji": True
                    },
                    "url": dashboard_url,
                    "style": "primary"
                }
            ]
        },
        {
            "type": "context",
            "elements": [
                {
                    "type": "mrkdwn",
                    "text": f"_{'ë¡œì»¬ í…ŒìŠ¤íŠ¸' if TEST_MODE else 'GitHub Actions'}ì—ì„œ ìƒì„±ë¨_"
                }
            ]
        }
    ])

    return {
        "attachments": [
            {
                "color": status_color,
                "blocks": blocks
            }
        ]
    }


def send_to_slack(message: Dict) -> bool:
    """Slackìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡"""
    if not SLACK_WEBHOOK:
        print("âš ï¸  SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•„ Slack ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return True

    if TEST_MODE:
        print("ğŸ” í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ì£¼ê°„ Slack ë©”ì‹œì§€ ë‚´ìš©:")
        print(json.dumps(message, indent=2, ensure_ascii=False))
        print("\nğŸ’¡ ì‹¤ì œ ì „ì†¡í•˜ë ¤ë©´ TEST_MODE=falseë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return True

    try:
        response = requests.post(SLACK_WEBHOOK, json=message)

        if response.status_code == 200:
            print("âœ… ì£¼ê°„ Slack ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ")
            return True
        else:
            print(f"âŒ ì£¼ê°„ Slack ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
            print(f"Response: {response.text}")
            return False
    except Exception as e:
        print(f"âŒ ì£¼ê°„ Slack ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("ğŸš€ ì£¼ê°„ í¬ë˜ì‹œ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")

        if TEST_MODE:
            print("ğŸ§ª ì£¼ê°„ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")

        # ì£¼ê°„ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        week_info = get_weekly_datetime_range()
        this_week_start_utc, this_week_end_utc, this_week_start_kst = week_info['this_week']
        prev_week_start_utc, prev_week_end_utc, prev_week_start_kst = week_info['prev_week']

        this_week_str = this_week_start_kst.strftime('%Y-%m-%d')
        prev_week_str = prev_week_start_kst.strftime('%Y-%m-%d')

        print(f"ğŸ“… ì´ë²ˆ ì£¼: {this_week_str} ~ {(this_week_start_kst + timedelta(days=6)).strftime('%Y-%m-%d')} (KST)")
        print(f"ğŸ“… ì „ì£¼: {prev_week_str} ~ {(prev_week_start_kst + timedelta(days=6)).strftime('%Y-%m-%d')} (KST)")

        # Sentry ì—°ê²° í…ŒìŠ¤íŠ¸ (TEST_MODEì¼ ë•Œë§Œ)
        if TEST_MODE:
            print("\nğŸ” Sentry ì—°ê²° í…ŒìŠ¤íŠ¸...")
            test_url = f"{SENTRY_API_BASE}/projects/{ORG_SLUG}/{PROJECT_SLUG}/"
            test_response = requests.get(test_url, headers=HEADERS)

            if test_response.status_code == 200:
                project_info = test_response.json()
                print(f"âœ… Sentry ì—°ê²° ì„±ê³µ: {project_info.get('name')} ({project_info.get('platform')})")
            else:
                print(f"âŒ Sentry ì—°ê²° ì‹¤íŒ¨: {test_response.status_code}")
                return

        # âœ¨ í•µì‹¬ ë³€ê²½: ìš”ì¼ë³„ ë°ì´í„° ë¨¼ì € ìˆ˜ì§‘
        print("\nğŸ“Š ì´ë²ˆ ì£¼ ì¼ë³„ í¬ë˜ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        this_week_daily = collect_daily_crash_data_simple(this_week_start_utc, "ì´ë²ˆì£¼")
        this_week_total = sum(this_week_daily)

        print("\nğŸ“Š ì „ì£¼ ì¼ë³„ í¬ë˜ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        prev_week_daily = collect_daily_crash_data_simple(prev_week_start_utc, "ì „ì£¼")
        prev_week_total = sum(prev_week_daily)

        print(f"\nğŸ“ˆ ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"  - ì´ë²ˆ ì£¼ ì´ê³„: {this_week_total}ê±´ (ì¼ë³„: {this_week_daily})")
        print(f"  - ì „ì£¼ ì´ê³„: {prev_week_total}ê±´ (ì¼ë³„: {prev_week_daily})")

        # ì´ë²ˆ ì£¼ ì´ìŠˆ ìˆ˜ì§‘ (ìƒëª…ì£¼ê¸° ë¶„ì„ìš©)
        print("\nğŸ“Š ì´ë²ˆ ì£¼ ì´ìŠˆ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        this_week_issues = collect_weekly_issues(this_week_start_utc, this_week_end_utc, "ì´ë²ˆì£¼")

        # ì „ì£¼ ì´ìŠˆ ìˆ˜ì§‘ (ìƒëª…ì£¼ê¸° ë¶„ì„ìš©)
        print("\nğŸ“Š ì „ì£¼ ì´ìŠˆ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        prev_week_issues = collect_weekly_issues(prev_week_start_utc, prev_week_end_utc, "ì „ì£¼")

        # ê°„ë‹¨í•œ í†µê³„ (ì´ìŠˆ ê°œìˆ˜ì™€ ì˜í–¥ ì‚¬ìš©ìëŠ” ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚°)
        this_week_crash_issues = []
        crash_issue_ids = set()  # ì¤‘ë³µ ì œê±°ìš©

        for issue in this_week_issues:
            level = issue.get('level', '').lower()
            issue_id = issue.get('id')
            # IDê°€ ìˆê³ , í¬ë˜ì‹œ ë ˆë²¨ì´ë©°, ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°ë§Œ
            if issue_id and level in ['error', 'fatal'] and issue_id not in crash_issue_ids:
                this_week_crash_issues.append(issue)
                crash_issue_ids.add(issue_id)

        # ì˜í–¥ë°›ì€ ì‚¬ìš©ì ìˆ˜ ì¶”ì • (ê° ì´ìŠˆì˜ userCount í•©ê³„, ì¤‘ë³µ ê³ ë ¤ ì•ˆí•¨)
        estimated_users = 0
        for issue in this_week_crash_issues[:50]:  # ìƒìœ„ 50ê°œë§Œ
            estimated_users += safe_int(issue.get('userCount', 0))

        this_week_stats = {
            'total_crashes': this_week_total,  # ìš”ì¼ë³„ í•©ê³„ ì‚¬ìš©
            'total_issues': len(crash_issue_ids),  # ê³ ìœ  í¬ë˜ì‹œ ì´ìŠˆ ê°œìˆ˜
            'affected_users': min(estimated_users, this_week_total),  # í¬ë˜ì‹œ ìˆ˜ë³´ë‹¤ ë§ì„ ìˆ˜ ì—†ìŒ
            'issues': this_week_crash_issues
        }

        print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
        print(f"  - ì´ë²ˆ ì£¼ í¬ë˜ì‹œ: {this_week_stats['total_crashes']}ê±´")
        print(f"  - ì´ë²ˆ ì£¼ í¬ë˜ì‹œ ì´ìŠˆ: {this_week_stats['total_issues']}ê°œ (ê³ ìœ  ID ê¸°ì¤€)")
        print(f"  - ì´ë²ˆ ì£¼ ì˜í–¥ ì‚¬ìš©ì: {this_week_stats['affected_users']}ëª… (ì¶”ì •)")

        if TEST_MODE:
            print(f"\nğŸ” ë””ë²„ê¹… ì •ë³´:")
            print(f"  - ì „ì²´ ì´ìŠˆ ìˆ˜: {len(this_week_issues)}ê°œ")
            print(f"  - í¬ë˜ì‹œ ì´ìŠˆ ìˆ˜: {len(this_week_crash_issues)}ê°œ")
            print(f"  - ê³ ìœ  ì´ìŠˆ ID ìˆ˜: {len(crash_issue_ids)}ê°œ")

        # ì´ìƒ ì§•í›„ íƒì§€
        print("\nğŸ” ì´ìƒ ì§•í›„ íƒì§€ ì¤‘...")
        anomalies = detect_anomalies_simple(this_week_daily)

        if anomalies:
            print(f"âš ï¸ {len(anomalies)}ê°œ ì´ìƒ ì§•í›„ ê°ì§€:")
            for anomaly in anomalies:
                print(f"  - {anomaly}")
        else:
            print("âœ… ì´ìƒ ì§•í›„ ì—†ìŒ")

        # ì´ìŠˆ ìƒëª…ì£¼ê¸° ë¶„ì„
        print("\nğŸ”„ ì´ìŠˆ ìƒëª…ì£¼ê¸° ë¶„ì„ ì¤‘...")
        lifecycle = analyze_issue_lifecycle_improved(
            this_week_issues, prev_week_issues,
            this_week_start_utc, this_week_end_utc,
            prev_week_start_utc, prev_week_end_utc
        )

        print(f"  - ì§„ì§œ ì‹ ê·œ ì´ìŠˆ: {len(lifecycle['new'])}ê°œ")
        print(f"  - ì•…í™” ì´ìŠˆ: {len(lifecycle['worsened'])}ê°œ")
        print(f"  - ê°œì„  ì´ìŠˆ: {len(lifecycle['improved'])}ê°œ")
        print(f"  - í•´ê²° ì´ìŠˆ: {len(lifecycle['resolved'])}ê°œ")

        # ì£¼ê°„ Crash-Free Rate ì¡°íšŒ
        print("\nğŸ“Š ì£¼ê°„ Crash-Free Rate ì¡°íšŒ ì¤‘...")
        crash_free_rate = get_weekly_crash_free_rate()
        print(f"  - ì£¼ê°„ Crash-Free Rate: {crash_free_rate}")

        # ìŠ¬ë™ ë©”ì‹œì§€ ìƒì„± (ìš”ì¼ë³„ í•©ê³„ ê¸°ì¤€)
        print("\nğŸ“ ì£¼ê°„ ë¦¬í¬íŠ¸ ë©”ì‹œì§€ ìƒì„± ì¤‘...")
        message = format_weekly_slack_message(
            this_week_total,  # ìš”ì¼ë³„ í•©ê³„
            prev_week_total,  # ì „ì£¼ ìš”ì¼ë³„ í•©ê³„
            this_week_stats,
            lifecycle,
            anomalies,
            crash_free_rate,
            week_info,
            this_week_daily
        )

        # Slack ì „ì†¡
        print("\nğŸ“¤ Slackìœ¼ë¡œ ì „ì†¡ ì¤‘...")
        success = send_to_slack(message)

        if success:
            print("\nğŸ‰ ì£¼ê°„ í¬ë˜ì‹œ ë¦¬í¬íŠ¸ ì „ì†¡ ì™„ë£Œ!")
            print(f"   - ì´ë²ˆ ì£¼: {this_week_total}ê±´ (ìš”ì¼ë³„ í•©ê³„)")
            print(f"   - ì „ì£¼: {prev_week_total}ê±´ (ìš”ì¼ë³„ í•©ê³„)")

            # ì‹¬ê°í•œ ìƒí™© ì•Œë¦¼
            if this_week_total > 500:
                print("âš ï¸ ì£¼ê°„ í¬ë˜ì‹œê°€ 500ê±´ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!")
            elif anomalies:
                print("âš ï¸ ì´ìƒ ì§•í›„ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            print("\nâŒ ì£¼ê°„ ë¦¬í¬íŠ¸ ì „ì†¡ ì‹¤íŒ¨")
            exit(1)

    except Exception as e:
        print(f"\nğŸ’¥ ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")

        if TEST_MODE:
            import traceback
            print("\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            traceback.print_exc()

        # ì˜¤ë¥˜ ì•Œë¦¼ë„ Slackìœ¼ë¡œ ì „ì†¡
        if SLACK_WEBHOOK and not TEST_MODE:
            error_message = {
                "text": f"ğŸš¨ ì£¼ê°„ í¬ë˜ì‹œ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"*ğŸš¨ ì£¼ê°„ í¬ë˜ì‹œ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜*\n\n"
                                    f"â€¢ ì˜¤ë¥˜: `{str(e)}`\n"
                                    f"â€¢ ì‹œê°„: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')} KST\n"
                                    f"â€¢ í™˜ê²½: {'ë¡œì»¬ í…ŒìŠ¤íŠ¸' if TEST_MODE else 'GitHub Actions'}"
                        }
                    }
                ]
            }

            if not TEST_MODE:
                error_message["blocks"][0]["text"]["text"] += f"\nâ€¢ ì €ì¥ì†Œ: `{os.getenv('GITHUB_REPOSITORY', 'unknown')}`"
                error_message["blocks"].append({
                    "type": "actions",
                    "elements": [
                        {
                            "type": "button",
                            "text": {
                                "type": "plain_text",
                                "text": "GitHub Actions ë¡œê·¸ í™•ì¸"
                            },
                            "url": f"https://github.com/{os.getenv('GITHUB_REPOSITORY', '')}/actions"
                        }
                    ]
                })

            send_to_slack(error_message)

        exit(1)


if __name__ == "__main__":
    main()