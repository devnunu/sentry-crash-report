#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sentry ì¼ì¼ ìš”ì•½(ì–´ì œ/ê·¸ì €ê»˜, í•œêµ­ì‹œê°„ ê¸°ì¤€) - REST API + Slack í¬ë§¤íŒ…/ì „ì†¡
- ì–´ì œ ìƒìœ„ 5ê°œ ì´ìŠˆ, ì‹ ê·œ ë°œìƒ ì´ìŠˆ(firstSeen ë‹¹ì¼), ê³ ê¸‰ ê¸‰ì¦ ì´ìŠˆ(DoD/7ì¼ ë² ì´ìŠ¤ë¼ì¸)
- Slack Webhookìœ¼ë¡œ ë¦¬í¬íŠ¸ ì „ì†¡ (SLACK_WEBHOOK_URLì´ ìˆì„ ë•Œ)
"""

import json
import os
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple

import math
import requests
from dotenv import load_dotenv

API_BASE = "https://sentry.io/api/0"

# ====== (ìƒìˆ˜) ê¸‰ì¦ íƒì§€ íŒŒë¼ë¯¸í„° ======
SURGE_MIN_COUNT = 30               # ì„œì§€ íŒì • ìµœì†Œ ë‹¹ì¼ ì´ë²¤íŠ¸ ìˆ˜
SURGE_GROWTH_MULTIPLIER = 2.0      # DoD ì„±ì¥ë°°ìœ¨ ì„ê³„ì¹˜ (ì˜ˆ: 2ë°°â†‘)
SURGE_Z_THRESHOLD = 2.0            # Z-score ì„ê³„ì¹˜
SURGE_MAD_THRESHOLD = 3.5          # Robust(MAD) ì„ê³„ì¹˜
SURGE_MIN_NEW_BURST = 15           # 7ì¼ ëª¨ë‘ 0ì¼ ë•Œ ë‹¹ì¼ í­ë°œë¡œ ê°„ì£¼í•˜ëŠ” ìµœì†Œì¹˜
BASELINE_DAYS = 7                  # ë² ì´ìŠ¤ë¼ì¸ ì¼ìˆ˜(ê·¸ì €ê»˜ í¬í•¨)
CANDIDATE_LIMIT = 100              # Discover per_page(ìµœëŒ€ 100). í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ë” ê°€ì ¸ì˜´
SURGE_MAX_RESULTS = 50             # ê²°ê³¼ ë°°ì—´ ìƒí•œ
SURGE_ABSOLUTE_MIN = SURGE_MIN_COUNT

# =========================
# Slack í¬ë§· ìƒìˆ˜ (í•œêµ­ì–´)
# =========================
SLACK_MAX_NEW = 5
SLACK_MAX_SURGE = 10
TITLE_MAX = 90
EMOJI_TOP = "ğŸ…"
EMOJI_NEW = "ğŸ†•"
EMOJI_SURGE = "ğŸ“ˆ"

# ----- íƒ€ì„ì¡´ -----
try:
    from zoneinfo import ZoneInfo  # py3.9+
except Exception:
    from backports.zoneinfo import ZoneInfo  # type: ignore

KST = ZoneInfo("Asia/Seoul")
UTC = timezone.utc


# ----- ê³µí†µ ìœ í‹¸ -----
def kst_day_bounds_utc_iso(day_kst_date: datetime) -> Tuple[str, str]:
    start_kst = day_kst_date.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=KST)
    end_kst = start_kst + timedelta(days=1)
    start_utc = start_kst.astimezone(UTC)
    end_utc = end_kst.astimezone(UTC)
    return start_utc.isoformat().replace("+00:00", "Z"), end_utc.isoformat().replace("+00:00", "Z")


def pretty_kst_date(d: datetime) -> str:
    return d.astimezone(KST).strftime("%Y-%m-%d")


def auth_headers(token: str) -> Dict[str, str]:
    return {"Authorization": f"Bearer {token}"}


def ensure_ok(r: requests.Response) -> requests.Response:
    try:
        r.raise_for_status()
    except requests.HTTPError as e:
        msg = f"HTTP {r.status_code} for {r.request.method} {r.url}\nResponse: {r.text[:800]}"
        raise SystemExit(msg) from e
    return r


# ----- í”„ë¡œì íŠ¸ ID -----
def resolve_project_id(token: str, org: str, project_slug: Optional[str], project_id_env: Optional[str]) -> int:
    if project_id_env:
        return int(project_id_env)
    if not project_slug:
        raise SystemExit("SENTRY_PROJECT_SLUG ë˜ëŠ” SENTRY_PROJECT_ID ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤.")
    url = f"{API_BASE}/organizations/{org}/projects/"
    r = ensure_ok(requests.get(url, headers=auth_headers(token), timeout=30))
    for p in r.json():
        if p.get("slug") == project_slug:
            return int(p.get("id"))
    raise SystemExit(f"'{project_slug}' í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# ----- Discover ì§‘ê³„ (ì „ì²´ ìš”ì•½) -----
def discover_aggregates_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str
) -> Dict[str, Any]:
    url = f"{API_BASE}/organizations/{org}/events/"
    query = "level:[error,fatal]" + (f" environment:{environment}" if environment else "")
    params = {
        "field": ["count()", "count_unique(issue)", "count_unique(user)"],
        "project": project_id,
        "start": start_iso_utc,
        "end": end_iso_utc,
        "query": query,
        "referrer": "api.summaries.daily",
    }
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    rows = (r.json().get("data") or [])
    if not rows:
        return {"crash_events": 0, "unique_issues": 0, "impacted_users": 0}
    row0 = rows[0]
    return {
        "crash_events": int(row0.get("count()") or 0),
        "unique_issues": int(row0.get("count_unique(issue)") or 0),
        "impacted_users": int(row0.get("count_unique(user)") or 0),
    }


# ----- Discover: ì´ìŠˆë³„ count() ë§µ (í˜ì´ì§€ë„¤ì´ì…˜) -----
def parse_link_cursor(link_header: str) -> Optional[str]:
    if 'rel="next"' in link_header and 'results="true"' in link_header:
        try:
            start = link_header.index("cursor=") + len("cursor=")
            end = link_header.index(">", start)
            return link_header[start:end]
        except Exception:
            return None
    return None


def issue_counts_map_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str, per_page: int = 100, max_pages: int = 10
) -> Dict[str, Dict[str, Any]]:
    """
    ì§€ì • ì¼ìì˜ ì´ìŠˆë³„ count()ë¥¼ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ìµœëŒ€ max_pagesê¹Œì§€ ìˆ˜ì§‘
    ë°˜í™˜: { issue_id: {"count": int, "title": str} }
    """
    url = f"{API_BASE}/organizations/{org}/events/"
    query = "level:[error,fatal]" + (f" environment:{environment}" if environment else "")
    headers = auth_headers(token)

    out: Dict[str, Dict[str, Any]] = {}
    cursor = None
    page = 0
    while True:
        page += 1
        params = {
            "field": ["issue", "title", "count()"],
            "project": project_id,
            "start": start_iso_utc,
            "end": end_iso_utc,
            "query": query,
            "orderby": "-count()",
            "per_page": per_page,  # 1~100
            "referrer": "api.summaries.issue-counts",
        }
        if cursor:
            params["cursor"] = cursor
        r = ensure_ok(requests.get(url, headers=headers, params=params, timeout=60))
        rows = (r.json().get("data") or [])
        for row in rows:
            iid = row.get("issue")
            if not iid:
                continue
            out[str(iid)] = {
                "count": int(row.get("count()") or 0),
                "title": row.get("title"),
            }
        link = r.headers.get("link", "")
        cursor = parse_link_cursor(link)
        if not cursor or page >= max_pages:
            break
    return out


# ----- ìƒìœ„ 5ê°œ ì´ìŠˆ -----
def top_issues_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str, limit: int = 5
) -> List[Dict[str, Any]]:
    url = f"{API_BASE}/organizations/{org}/events/"
    query = "level:[error,fatal]" + (f" environment:{environment}" if environment else "")
    params = {
        "field": ["issue", "title", "count()"],
        "project": project_id,
        "start": start_iso_utc,
        "end": end_iso_utc,
        "query": query,
        "orderby": "-count()",
        "per_page": limit,
        "referrer": "api.summaries.top-issues",
    }
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    rows = (r.json().get("data") or [])
    return [
        {
            "issue_id": row.get("issue"),
            "title": row.get("title"),
            "event_count": row.get("count()"),
            "link": f"https://sentry.io/organizations/{org}/issues/{row.get('issue')}/" if row.get("issue") else None,
        }
        for row in rows[:limit]
    ]


# ----- ì‹ ê·œ ë°œìƒ ì´ìŠˆ (Issues API: firstSeen) -----
def new_issues_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str
) -> List[Dict[str, Any]]:
    url = f"{API_BASE}/organizations/{org}/issues/"
    q_parts = [f"firstSeen:>={start_iso_utc}", f"firstSeen:<{end_iso_utc}", "level:[error,fatal]"]
    if environment:
        q_parts.append(f"environment:{environment}")
    query = " ".join(q_parts)
    params = {
        "project": project_id,
        "since": start_iso_utc,
        "until": end_iso_utc,
        "query": query,
        "sort": "date",
        "per_page": 100,
        "referrer": "api.summaries.new-issues",
    }
    headers = auth_headers(token)
    results: List[Dict[str, Any]] = []
    cursor: Optional[str] = None

    while True:
        if cursor:
            params["cursor"] = cursor
        r = ensure_ok(requests.get(url, headers=headers, params=params, timeout=60))
        items = r.json() or []
        for it in items:
            iid = it.get("id")
            permalink = it.get("permalink") or (f"https://sentry.io/organizations/{org}/issues/{iid}/" if iid else None)
            results.append({
                "issue_id": iid,
                "title": it.get("title"),
                "event_count": int(it.get("count")) if it.get("count") is not None else None,
                "first_seen": it.get("firstSeen"),
                "link": permalink,
            })
        link = r.headers.get("link", "")
        cursor = parse_link_cursor(link)
        if not cursor:
            break

    return results


# ====== í†µê³„ ìœ í‹¸ ======
def mean_std(values: List[int]) -> Tuple[float, float]:
    if not values:
        return 0.0, 0.0
    m = sum(values) / len(values)
    var = sum((v - m) ** 2 for v in values) / max(len(values), 1)
    return m, math.sqrt(var)


def median(values: List[int]) -> float:
    if not values:
        return 0.0
    s = sorted(values)
    n = len(s)
    mid = n // 2
    if n % 2 == 1:
        return float(s[mid])
    return (s[mid - 1] + s[mid]) / 2.0


def mad(values: List[int], med: Optional[float] = None) -> float:
    if not values:
        return 0.0
    m = med if med is not None else median(values)
    dev = [abs(v - m) for v in values]
    return median(dev)


# ====== ê³ ê¸‰ ê¸‰ì¦ íƒì§€ ======
def detect_surge_issues_advanced(
    token: str, org: str, project_id: int, environment: Optional[str],
    target_start_utc: str, target_end_utc: str,
    baseline_days: int = BASELINE_DAYS,
    per_page: int = 100, max_pages: int = 10
) -> List[Dict[str, Any]]:
    # íƒ€ê²Ÿì¼ ì´ìŠˆë³„ ì¹´ìš´íŠ¸(í˜ì´ì§€ë„¤ì´ì…˜)
    today_map = issue_counts_map_for_day(
        token, org, project_id, environment,
        target_start_utc, target_end_utc, per_page, max_pages
    )

    # ì§ì „ Nì¼ ë§µë“¤
    def iso_to_dt(iso_str: str) -> datetime:
        return datetime.fromisoformat(iso_str.replace("Z", "+00:00")).astimezone(UTC)
    t_start_dt = iso_to_dt(target_start_utc)

    prev_maps: List[Dict[str, Dict[str, Any]]] = []
    for i in range(1, baseline_days + 1):
        day_start_dt = t_start_dt - timedelta(days=i)
        day_end_dt = day_start_dt + timedelta(days=1)
        start_iso = day_start_dt.isoformat().replace("+00:00", "Z")
        end_iso   = day_end_dt.isoformat().replace("+00:00", "Z")
        prev_maps.append(
            issue_counts_map_for_day(token, org, project_id, environment, start_iso, end_iso, per_page, max_pages)
        )

    results: List[Dict[str, Any]] = []
    eps = 1e-9

    for iid, cur_info in today_map.items():
        # ë°©ì–´: íƒ€ì… ì„ì„ ëŒ€ë¹„
        try:
            cur = int(cur_info.get("count") or 0)
        except Exception:
            cur = 0

        # 1ì°¨ í•„í„°: ì ˆëŒ€ ìµœì†Œ ê±´ìˆ˜(ì–´ë–¤ ì¡°ê±´ì´ë“  ì´ ê°’ ë¯¸ë§Œì´ë©´ ì œì™¸)
        if cur < SURGE_ABSOLUTE_MIN:
            continue

        title = cur_info.get("title")
        link  = f"https://sentry.io/organizations/{org}/issues/{iid}/"

        # D-1 ë° ë² ì´ìŠ¤ë¼ì¸
        dby = int(prev_maps[0].get(iid, {}).get("count") or 0) if prev_maps else 0
        baseline_counts = [int(pm.get(iid, {}).get("count") or 0) for pm in prev_maps]

        mean_val, std_val = mean_std(baseline_counts)
        med_val           = median(baseline_counts)
        mad_val           = mad(baseline_counts, med_val)

        z = (cur - mean_val) / (std_val + eps) if std_val > 0 else (float("inf") if cur > mean_val else 0.0)
        mad_score = (cur - med_val) / (1.4826 * mad_val + eps) if mad_val > 0 else (float("inf") if cur > med_val else 0.0)
        growth = cur / max(dby, 1)

        is_all_zero = all(v == 0 for v in baseline_counts)
        conditions = {
            "growth":   growth >= SURGE_GROWTH_MULTIPLIER,
            "zscore":   z >= SURGE_Z_THRESHOLD,
            "madscore": mad_score >= SURGE_MAD_THRESHOLD,
            # ì™„ì „ ì‹ ê·œ í­ë°œ: ê·¸ë˜ë„ curëŠ” ìœ„ì˜ SURGE_ABSOLUTE_MINì„ ì´ë¯¸ í†µê³¼í•´ì•¼ í•¨
            "new_burst": (is_all_zero and cur >= max(SURGE_MIN_NEW_BURST, SURGE_ABSOLUTE_MIN)),
        }

        if any(conditions.values()):
            results.append({
                "issue_id": iid,
                "title": title,
                "event_count": cur,
                "link": link,
                "dby_count": dby,
                "growth_multiplier": round(growth, 2),
                "zscore": None if math.isinf(z) else round(z, 2),
                "mad_score": None if math.isinf(mad_score) else round(mad_score, 2),
                "baseline_mean": round(mean_val, 2),
                "baseline_std": round(std_val, 2),
                "baseline_median": round(med_val, 2),
                "baseline_mad": round(mad_val, 2),
                "baseline_counts": baseline_counts,
                "reasons": [k for k, v in conditions.items() if v],
            })

    # 2ì°¨ ë³´ì •: í˜¹ì‹œë¼ë„ ê³„ì‚°/íƒ€ì… ì´ìŠˆë¡œ í†µê³¼í•œ í•­ëª©ì„ ë‹¤ì‹œ ì ˆëŒ€ ìµœì†Œê±´ìˆ˜ë¡œ ê±¸ëŸ¬ëƒ„
    results = [r for r in results if int(r.get("event_count") or 0) >= SURGE_ABSOLUTE_MIN]

    # ì •ë ¬/ìƒí•œ
    results.sort(
        key=lambda x: (x["event_count"], (x["zscore"] or 0), (x["mad_score"] or 0), x["growth_multiplier"]),
        reverse=True
    )
    return results[:SURGE_MAX_RESULTS]


# ----- Crash Free ë©”íŠ¸ë¦­ -----
def sessions_crash_free_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str
) -> Tuple[Optional[float], Optional[float]]:
    url = f"{API_BASE}/organizations/{org}/sessions/"
    params = {
        "project": project_id,
        "start": start_iso_utc,
        "end": end_iso_utc,
        "interval": "1d",
        "field": ["crash_free_rate(session)", "crash_free_rate(user)"],
        "referrer": "api.summaries.daily",
    }
    if environment:
        params["environment"] = environment
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    data = r.json()
    cf_s, cf_u = None, None
    for g in data.get("groups", []):
        series = g.get("series", {})
        if "crash_free_rate(session)" in series and series["crash_free_rate(session)"]:
            cf_s = float(series["crash_free_rate(session)"][-1])
        if "crash_free_rate(user)" in series and series["crash_free_rate(user)"]:
            cf_u = float(series["crash_free_rate(user)"][-1])
    return cf_s, cf_u


# =========================
# AI ì¡°ì–¸ ìƒì„± (OpenAI)
# =========================
def generate_ai_advice(summary_payload: Dict[str, Any], y_key: str, dby_key: Optional[str], environment: Optional[str]) -> Dict[str, Any]:
    """
    summary_payload: main()ì—ì„œ ë§Œë“  result ì „ì²´(dict)
    y_key: ì–´ì œ ë‚ ì§œ í‚¤ ("YYYY-MM-DD")
    dby_key: ê·¸ì €ê»˜ ë‚ ì§œ í‚¤ ë˜ëŠ” None
    environment: í™˜ê²½ëª… (ì˜ˆ: Production)
    ë°˜í™˜: êµ¬ì¡°í™”ëœ dict (ëª¨ë¸ì´ JSON ë¬¸ìë¥¼ ë°˜í™˜í•˜ë©´ íŒŒì‹±)
    ì‹¤íŒ¨ ì‹œ: {"fallback_text": "..."} í˜•íƒœ
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return {"fallback_text": "AI ì¡°ì–¸ì„ ìƒì„±í•˜ë ¤ë©´ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤."}

    try:
        from openai import OpenAI
    except Exception:
        return {"fallback_text": "openai íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install openai"}

    client = OpenAI(api_key=api_key)

    # í”„ë¡¬í”„íŠ¸: í•œêµ­ì–´ + í–‰ë™ì§€í–¥ + ìš°ë¦¬ ìš”ì•½ JSON ê·¸ëŒ€ë¡œ ì œê³µ
    prompt = {
        "role": "user",
        "content": (
            "ë‹¹ì‹ ì€ ëª¨ë°”ì¼/ë°±ì—”ë“œ í¬ë˜ì‹œ í’ˆì§ˆ ì½”ì¹˜ì…ë‹ˆë‹¤. ë‹¤ìŒ Sentry ì¼ê°„ ìš”ì•½(JSON)ì„ ë°”íƒ•ìœ¼ë¡œ íŒ€ì— ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì£¼ëŠ” ì¡°ì–¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
            "- ì˜¤ëŠ˜ ë‹¹ì¥ ì‹¤í–‰í•  ì•¡ì…˜ 1~3ê°œ(ë‹´ë‹¹ ì—­í• /ì´ìœ  í¬í•¨)\n"
            "- ì¶”ê°€ ëª¨ë‹ˆí„°ë§ í•­ëª©(ì§€í‘œ/í•„í„°/ë¦´ë¦¬ì¦ˆ/OS/ë””ë°”ì´ìŠ¤ ë“±)\n"
            "- ì›ì¸ ì¶”ì • ë° ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸\n"
            "- ë¡œê·¸Â·ê³„ì¸¡(ì¶”ê°€ ìˆ˜ì§‘) ì œì•ˆ\n"
            "- ìƒìœ„ ì´ìŠˆë³„ ì½”ë©˜íŠ¸(í•„ìš” ì‹œë§Œ, 1~2ì¤„)\n"
            "- ê³¼ì¥ í‘œí˜„ ì—†ì´ ê·¼ê±° ì§€í‘œë¥¼ ê°„ë‹¨íˆ ì¨ ì£¼ì„¸ìš”.\n"
            "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONìœ¼ë¡œ:\n"
            "{\n"
            "  \"today_actions\": [ {\"title\":\"\", \"why\":\"\", \"owner_role\":\"\", \"suggestion\":\"\"} ],\n"
            "  \"monitoring\": [\"\"],\n"
            "  \"root_cause\": [\"\"],\n"
            "  \"logging\": [\"\"],\n"
            "  \"per_issue_notes\": [ {\"issue_title\":\"\", \"note\":\"\"} ]\n"
            "}\n\n"
            f"í™˜ê²½: {environment or 'N/A'}\n"
            f"ìš”ì•½(JSON):\n{json.dumps(summary_payload, ensure_ascii=False)}\n"
            f"ì–´ì œ í‚¤: {y_key}\nê·¸ì €ê»˜ í‚¤: {dby_key or 'ì—†ìŒ'}\n"
        )
    }

    try:
        # ìµœì‹  SDK: Responses API ì‚¬ìš© (í•„ìš”ì‹œ gpt-4o-mini ë“± ë³€ê²½ ê°€ëŠ¥)
        resp = client.responses.create(
            model="gpt-4o-mini",
            temperature=0.2,
            max_output_tokens=800,
            input=[prompt],
        )
        text = resp.output_text  # SDKê°€ ì œê³µí•˜ëŠ” í¸ì˜ ì ‘ê·¼ì
        # JSON íŒŒì‹± ì‹œë„
        data = json.loads(text)
        # ìµœì†Œ í‚¤ ë³´ì •
        for k in ["today_actions", "monitoring", "root_cause", "logging", "per_issue_notes"]:
            data.setdefault(k, [])
        return data
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œë¼ë„ ë°˜í™˜
        fallback = str(e)
        try:
            # í˜¹ì‹œ ëª¨ë¸ì´ JSON ë¹„ìŠ·í•œ ê±¸ ì¤¬ë‹¤ë©´ ëŠìŠ¨í•˜ê²Œ ì¬ì‹œë„
            import re
            m = re.search(r"\{.*\}", text, re.DOTALL)  # type: ignore
            if m:
                data = json.loads(m.group(0))
                for k in ["today_actions", "monitoring", "root_cause", "logging", "per_issue_notes"]:
                    data.setdefault(k, [])
                return data
        except Exception:
            pass
        return {"fallback_text": f"AI ì¡°ì–¸ ìƒì„± ì‹¤íŒ¨: {fallback[:200]}"}  # 200ì ì œí•œ


# ====== Slack ë©”ì‹œì§€ ë¹Œë”/ì „ì†¡ ======
# ---------- ë„ìš°ë¯¸ ----------
def truncate(s: Optional[str], n: int) -> Optional[str]:
    if s is None:
        return None
    return s if len(s) <= n else s[: n - 1] + "â€¦"

def fmt_pct(v: Optional[float]) -> str:
    if v is None:
        return "N/A"
    pct = v * 100
    truncated = int(pct * 100) / 100  # ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ ì ˆì‚­
    return f"{truncated:.2f}%"

def parse_iso_to_kst_label(start_utc_iso: str, end_utc_iso: str) -> str:
    """UTC ISO êµ¬ê°„ì„ í•œêµ­ì‹œ(KST)ë¡œ ë°”ê¿” ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ í‘œê¸°"""
    def to_kst(iso_s: str) -> datetime:
        return datetime.fromisoformat(iso_s.replace("Z", "+00:00")).astimezone(KST)
    s = to_kst(start_utc_iso)
    e = to_kst(end_utc_iso)
    # ì˜ˆ: 2025-09-01 00:00 ~ 2025-09-01 23:59 (KST)
    s_txt = s.strftime("%Y-%m-%d %H:%M")
    e_txt = e.strftime("%Y-%m-%d %H:%M")
    return f"{s_txt} ~ {e_txt} (KST)"

def diff_str(cur: int, prev: int, suffix: str = "ê±´") -> str:
    delta = cur - prev
    if delta > 0:
        arrow = "ğŸ”º"
    elif delta < 0:
        arrow = "ğŸ”»"
    else:
        arrow = "â€”"
    ratio = ""
    if prev > 0:
        ratio = f" ({(delta/prev)*100:+.1f}%)"
    return f"{cur}{suffix} {arrow}{abs(delta)}{suffix}{ratio}"

# ---------- ì´ìŠˆ ë¼ì¸(í•œêµ­ì–´) ----------
def issue_line_kr(item: Dict[str, Any]) -> str:
    """ì œëª©ì—ë§Œ ë§í¬, ì´ìŠˆí‚¤(#... ) ì œê±°, ê°œìˆ˜ëŠ” '7ê±´'ìœ¼ë¡œ í‘œê¸°"""
    title = truncate(item.get("title"), TITLE_MAX) or "(ì œëª© ì—†ìŒ)"
    link = item.get("link")
    count = item.get("event_count")
    count_txt = f"{int(count)}ê±´" if isinstance(count, (int, float)) and count is not None else "â€“"
    title_link = f"<{link}|{title}>" if link else title
    return f"â€¢ {title_link} Â· {count_txt}"

# ---------- ê¸‰ì¦ ì´ìŠˆ ì„¤ëª…(ì„œìˆ í˜•) ----------
def surge_explanation_kr(item: Dict[str, Any]) -> str:
    """
    ì˜ˆì‹œ:
    â€¢ Login NPE Â· 42ê±´
      â†³ ì „ì¼ 0ê±´ â†’ ì–´ì œ 42ê±´ìœ¼ë¡œ ê¸‰ì¦. ìµœê·¼ 7ì¼ í‰ê·  5.3ê±´/ì¤‘ì•™ê°’ 4ê±´ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€.
      â†³ íŒì • ê·¼ê±°: growth/madscore
    """
    base = issue_line_kr(item)
    cur = item.get("event_count") or 0
    d1 = item.get("dby_count") or 0
    mean_v = item.get("baseline_mean")
    med_v = item.get("baseline_median")
    reasons = item.get("reasons", [])
    # ì„œìˆ : ì „ì¼ ëŒ€ë¹„, 7ì¼ í‰ê· /ì¤‘ì•™ê°’ ëŒ€ë¹„
    parts = []
    parts.append(f"ì „ì¼ {d1}ê±´ â†’ ì–´ì œ {cur}ê±´ìœ¼ë¡œ ê¸‰ì¦.")
    if isinstance(mean_v, (int, float)) and isinstance(med_v, (int, float)):
        parts.append(f"ìµœê·¼ 7ì¼ í‰ê·  {mean_v:.1f}ê±´/ì¤‘ì•™ê°’ {med_v:.0f}ê±´ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€.")
    # ê·œì¹™ëª…ë§Œ ê°„ë‹¨ í‘œê¸°
    if reasons:
        ko = {
            "growth": "ì „ì¼ ëŒ€ë¹„ ê¸‰ì¦",
            "zscore": "í‰ê·  ëŒ€ë¹„ í†µê³„ì  ê¸‰ì¦",
            "madscore": "ì¤‘ì•™ê°’ ëŒ€ë¹„ ì´ìƒì¹˜",
            "new_burst": "ìµœê·¼ ê¸°ë¡ ê±°ì˜ ì—†ìŒì—ì„œ í­ì¦",
        }
        pretty = [ko.get(r, r) for r in reasons]
        parts.append("íŒì • ê·¼ê±°: " + "/".join(pretty))
    detail = "  â†³ " + " ".join(parts)
    return f"{base}\n{detail}"

# ---------- í•œêµ­ì–´ ë¸”ë¡ ë¹Œë” ----------
def build_ai_advice_blocks(ai: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    AI ê²°ê³¼ dict â†’ Slack Blocks
    íƒ€ì´í‹€: ":brain: AI ë¶„ì„ ì½”ë©˜íŠ¸"
    'ì¶”ê°€ ëª¨ë‹ˆí„°ë§', 'ë¡œê·¸Â·ê³„ì¸¡ ì œì•ˆ' ì„¹ì…˜ì€ ì œì™¸
    """
    blocks: List[Dict[str, Any]] = []
    blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:brain: AI ë¶„ì„ ì½”ë©˜íŠ¸*"}})

    # ì‹¤íŒ¨/í´ë°± í…ìŠ¤íŠ¸
    if "fallback_text" in ai:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": ai["fallback_text"]}})
        blocks.append({"type": "divider"})
        return blocks

    def bullets(label_emoji: str, title: str, items: List[Any]) -> Optional[Dict[str, Any]]:
        if not items:
            return None
        if isinstance(items[0], str):
            lines = "\n".join(f"â€¢ {x}" for x in items)
        else:
            if title == "ì˜¤ëŠ˜ì˜ ì•¡ì…˜":
                lines = "\n".join(
                    f"â€¢ *{x.get('title','(ì œëª© ì—†ìŒ)')}* â€” {x.get('suggestion','')}"
                    f"{' _(ë‹´ë‹¹: ' + x.get('owner_role','') + ', ì´ìœ : ' + x.get('why','') + ')_' if (x.get('owner_role') or x.get('why')) else ''}"
                    for x in items
                )
            elif title == "ì´ìŠˆë³„ ì½”ë©˜íŠ¸":
                lines = "\n".join(
                    f"â€¢ *{x.get('issue_title','(ì œëª© ì—†ìŒ)')}* â€” {x.get('note','')}"
                    for x in items
                )
            else:
                lines = "\n".join(f"â€¢ {x}" for x in items)
        return {"type": "section", "text": {"type": "mrkdwn", "text": f"*{label_emoji} {title}*\n{lines}"}}

    # ì˜¤ëŠ˜ì˜ ì•¡ì…˜
    sec = bullets(":memo:", "ì˜¤ëŠ˜ì˜ ì•¡ì…˜", ai.get("today_actions", []))
    if sec: blocks.append(sec)

    # ì›ì¸ ì¶”ì •Â·ì ê²€
    sec = bullets(":toolbox:", "ì›ì¸ ì¶”ì •Â·ì ê²€", ai.get("root_cause", []))
    if sec: blocks.append(sec)

    # ì´ìŠˆë³„ ì½”ë©˜íŠ¸
    sec = bullets(":speech_balloon:", "ì´ìŠˆë³„ ì½”ë©˜íŠ¸", ai.get("per_issue_notes", []))
    if sec: blocks.append(sec)

    blocks.append({"type": "divider"})
    return blocks

def build_slack_blocks_for_day(
    date_label: str,
    env_label: Optional[str],
    day_obj: Dict[str, Any],
    prev_day_obj: Optional[Dict[str, Any]] = None,
    ai_blocks: Optional[List[Dict[str, Any]]] = None,
) -> List[Dict[str, Any]]:
    # í˜„ì¬ê°’
    cf_s = day_obj.get("crash_free_sessions_pct")
    cf_u = day_obj.get("crash_free_users_pct")
    events = int(day_obj.get("crash_events", 0))
    issues = int(day_obj.get("unique_issues", 0))
    users  = int(day_obj.get("impacted_users", 0))

    # ì „ì¼ê°’ (ì¦ê°ì€ ì´ë²¤íŠ¸/ì´ìŠˆ/ì‚¬ìš©ìì—ë§Œ ì ìš©)
    prev_events = prev_issues = prev_users = 0
    if prev_day_obj:
        prev_events = int(prev_day_obj.get("crash_events", 0))
        prev_issues = int(prev_day_obj.get("unique_issues", 0))
        prev_users  = int(prev_day_obj.get("impacted_users", 0))

    # Summary: ìš”ì²­í•˜ì‹  ìˆœì„œë¡œ í‘œê¸° (ì´ë²¤íŠ¸/ì´ìŠˆ/ì‚¬ìš©ì â†’ Crash Free)
    summary_lines = [
        "*:memo: Summary*",
        f"â€¢ ğŸ’¥ *ì´ë²¤íŠ¸*: {diff_str(events, prev_events, suffix='ê±´') if prev_day_obj else f'{events}ê±´'}",
        f"â€¢ ğŸ *ì´ìŠˆ*: {diff_str(issues, prev_issues, suffix='ê±´') if prev_day_obj else f'{issues}ê±´'}",
        f"â€¢ ğŸ‘¥ *ì˜í–¥ ì‚¬ìš©ì*: {diff_str(users, prev_users, suffix='ëª…') if prev_day_obj else f'{users}ëª…'}",
        f"â€¢ ğŸ›¡ï¸ *Crash Free ì„¸ì…˜*: {fmt_pct(cf_s)}",
        f"â€¢ ğŸ›¡ï¸ *Crash Free ì‚¬ìš©ì*: {fmt_pct(cf_u)}",
    ]
    kpi_text = "\n".join(summary_lines)

    # ì§‘ê³„ êµ¬ê°„(KST)
    win = day_obj.get("window_utc") or {}
    kst_window = parse_iso_to_kst_label(win.get("start","?"), win.get("end","?"))

    # í—¤ë”
    title = f"Sentry ì¼ê°„ ë¦¬í¬íŠ¸ â€” {date_label}"
    if env_label:
        title += f"  Â·  {env_label}"

    blocks: List[Dict[str, Any]] = [
        {"type": "header", "text": {"type": "plain_text", "text": title, "emoji": True}},
        {"type": "section", "text": {"type": "mrkdwn", "text": kpi_text}},
        {"type": "context", "elements": [{"type": "mrkdwn", "text": f"*ì§‘ê³„ êµ¬ê°„*: {kst_window}"}]},
        {"type": "divider"},
    ]

    # === ì—¬ê¸°ì„œ AI ì„¹ì…˜ ì‚½ì… ===
    if ai_blocks:
        blocks.extend(ai_blocks)

    # ì•„ë˜ëŠ” ê¸°ì¡´ ì„¹ì…˜: íƒ€ì´í‹€ì€ ì´ëª¨ì§€ + êµµê²Œ ìœ ì§€
    top = day_obj.get("top_5_issues") or []
    if top:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:sports_medal: ìƒìœ„ 5ê°œ ì´ìŠˆ*"}})
        lines = "\n".join(issue_line_kr(x) for x in top)
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": lines}})
        blocks.append({"type": "divider"})

    new_issues = (day_obj.get("new_issues") or [])[:SLACK_MAX_NEW]
    if new_issues:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:new: ì‹ ê·œ ë°œìƒ ì´ìŠˆ*"}})
        lines = "\n".join(issue_line_kr(x) for x in new_issues)
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": lines}})
        blocks.append({"type": "divider"})

    surge = [x for x in (day_obj.get("surge_issues") or []) if int(x.get("event_count") or 0) >= SURGE_ABSOLUTE_MIN]
    surge = surge[:SLACK_MAX_SURGE]
    if surge:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:chart_with_upwards_trend: ê¸‰ì¦(ì„œì§€) ì´ìŠˆ*"}})
        lines = "\n".join(surge_explanation_kr(x) for x in surge)
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": lines}})
        blocks.append({"type": "divider"})

    return blocks

# ---------- Slack ì „ì†¡ ----------
def post_to_slack(webhook_url: str, blocks: List[Dict[str, Any]]) -> None:
    payload = {"blocks": blocks}
    r = requests.post(webhook_url, headers={"Content-Type": "application/json"}, data=json.dumps(payload), timeout=30)
    try:
        r.raise_for_status()
    except requests.HTTPError as e:
        print(f"[Slack] Post failed {r.status_code}: {r.text[:300]}")
        raise


# ====== ë©”ì¸ ======
def main():
    load_dotenv()
    token = os.getenv("SENTRY_AUTH_TOKEN") or ""
    org = os.getenv("SENTRY_ORG_SLUG") or ""
    project_slug = os.getenv("SENTRY_PROJECT_SLUG")
    project_id_env = os.getenv("SENTRY_PROJECT_ID")
    environment = os.getenv("SENTRY_ENVIRONMENT")
    slack_webhook = os.getenv("SLACK_WEBHOOK_URL")

    if not token or not org:
        raise SystemExit("SENTRY_AUTH_TOKEN, SENTRY_ORG_SLUG í•„ìˆ˜")

    # ë‚ ì§œ ë²”ìœ„(ì–´ì œ/ê·¸ì €ê»˜, KST â†’ UTC)
    now_kst = datetime.now(KST)
    y_kst = (now_kst - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    dby_kst = (now_kst - timedelta(days=2)).replace(hour=0, minute=0, second=0, microsecond=0)
    y_start, y_end = kst_day_bounds_utc_iso(y_kst)
    dby_start, dby_end = kst_day_bounds_utc_iso(dby_kst)

    # í”„ë¡œì íŠ¸ ID
    project_id = resolve_project_id(token, org, project_slug, project_id_env)

    # --- ì–´ì œ ë°ì´í„° ---
    y_summary = discover_aggregates_for_day(token, org, project_id, environment, y_start, y_end)
    y_cf_s, y_cf_u = sessions_crash_free_for_day(token, org, project_id, environment, y_start, y_end)
    y_top = top_issues_for_day(token, org, project_id, environment, y_start, y_end)
    y_new = new_issues_for_day(token, org, project_id, environment, y_start, y_end)
    y_surge_adv = detect_surge_issues_advanced(token, org, project_id, environment, y_start, y_end)

    # --- ê·¸ì €ê»˜ ë°ì´í„° (ë¹„êµìš©/ì¶œë ¥ í¬í•¨) ---
    dby_summary = discover_aggregates_for_day(token, org, project_id, environment, dby_start, dby_end)
    dby_cf_s, dby_cf_u = sessions_crash_free_for_day(token, org, project_id, environment, dby_start, dby_end)

    result = {
        "timezone": "Asia/Seoul (KST)",
        pretty_kst_date(y_kst): {
            **y_summary,
            "issues_count": y_summary["unique_issues"],
            "unique_issues_in_events": y_summary["unique_issues"],
            "crash_free_sessions_pct": y_cf_s,
            "crash_free_users_pct": y_cf_u,
            "top_5_issues": y_top,
            "new_issues": y_new,
            "surge_issues": y_surge_adv,
            "window_utc": {"start": y_start, "end": y_end},
        },
        pretty_kst_date(dby_kst): {
            **dby_summary,
            "issues_count": dby_summary["unique_issues"],
            "unique_issues_in_events": dby_summary["unique_issues"],
            "crash_free_sessions_pct": dby_cf_s,
            "crash_free_users_pct": dby_cf_u,
            "window_utc": {"start": dby_start, "end": dby_end},
        },
    }

    # ì½˜ì†” ì¶œë ¥
    print(json.dumps(result, ensure_ascii=False, indent=2))

    if slack_webhook:
        y_key = pretty_kst_date(y_kst)
        dby_key = pretty_kst_date(dby_kst)

        # === AI ì¡°ì–¸ ìƒì„± ===
        ai_data = generate_ai_advice(result, y_key=y_key, dby_key=dby_key, environment=environment)
        ai_blocks = build_ai_advice_blocks(ai_data)

        # ì–´ì œ ë¸”ë¡ ìƒì„± (AI ë¸”ë¡ ì‚½ì…)
        y_blocks = build_slack_blocks_for_day(
            date_label=y_key,
            env_label=environment,
            day_obj=result[y_key],
            prev_day_obj=result.get(dby_key),
            ai_blocks=ai_blocks,  # â† ì—¬ê¸°!
        )

        try:
            post_to_slack(slack_webhook, y_blocks)
            print("[Slack] ì–´ì œ ë¦¬í¬íŠ¸ ì „ì†¡ ì™„ë£Œ (AI í¬í•¨).")
        except Exception as e:
            print(f"[Slack] ì „ì†¡ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()