#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sentry ì¼ì¼ ìš”ì•½(ì–´ì œ/ê·¸ì €ê»˜, í•œêµ­ì‹œê°„ ê¸°ì¤€) - REST API + Slack í¬ë§¤íŒ…/ì „ì†¡
- ì–´ì œ ìƒìœ„ 5ê°œ ì´ìŠˆ, ì‹ ê·œ ë°œìƒ ì´ìŠˆ(firstSeen ë‹¹ì¼), ê³ ê¸‰ ê¸‰ì¦ ì´ìŠˆ(DoD/7ì¼ ë² ì´ìŠ¤ë¼ì¸)
- Slack Webhookìœ¼ë¡œ ë¦¬í¬íŠ¸ ì „ì†¡ (SLACK_WEBHOOK_URLì´ ìˆì„ ë•Œ)
"""

import json
import os
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple

import math
import requests
from dotenv import load_dotenv

API_BASE = "https://sentry.io/api/0"

# ====== (ìƒìˆ˜) ê¸‰ì¦ íƒì§€ íŒŒë¼ë¯¸í„° ======
SURGE_MIN_COUNT = 30               # ì„œì§€ íŒì • ìµœì†Œ ë‹¹ì¼ ì´ë²¤íŠ¸ ìˆ˜
SURGE_GROWTH_MULTIPLIER = 2.0      # DoD ì„±ì¥ë°°ìœ¨ ì„ê³„ì¹˜ (ì˜ˆ: 2ë°°â†‘)
SURGE_Z_THRESHOLD = 2.0            # Z-score ì„ê³„ì¹˜
SURGE_MAD_THRESHOLD = 3.5          # Robust(MAD) ì„ê³„ì¹˜
SURGE_MIN_NEW_BURST = 15           # 7ì¼ ëª¨ë‘ 0ì¼ ë•Œ ë‹¹ì¼ í­ë°œë¡œ ê°„ì£¼í•˜ëŠ” ìµœì†Œì¹˜
BASELINE_DAYS = 7                  # ë² ì´ìŠ¤ë¼ì¸ ì¼ìˆ˜(ê·¸ì €ê»˜ í¬í•¨)
CANDIDATE_LIMIT = 100              # Discover per_page(ìµœëŒ€ 100). í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ë” ê°€ì ¸ì˜´
SURGE_MAX_RESULTS = 50             # ê²°ê³¼ ë°°ì—´ ìƒí•œ
SURGE_ABSOLUTE_MIN = SURGE_MIN_COUNT

# =========================
# Slack í¬ë§· ìƒìˆ˜ (í•œêµ­ì–´)
# =========================
SLACK_MAX_NEW = 5
SLACK_MAX_SURGE = 10
TITLE_MAX = 90
EMOJI_TOP = "ğŸ…"
EMOJI_NEW = "ğŸ†•"
EMOJI_SURGE = "ğŸ“ˆ"

# ----- íƒ€ì„ì¡´ -----
try:
    from zoneinfo import ZoneInfo  # py3.9+
except Exception:
    from backports.zoneinfo import ZoneInfo  # type: ignore

KST = ZoneInfo("Asia/Seoul")
UTC = timezone.utc

# ----- ë¡œê·¸ ìœ í‹¸ -----
def log(msg: str) -> None:
    print(f"[Daily] {msg}")

# ----- ê³µí†µ ìœ í‹¸ -----
def kst_day_bounds_utc_iso(day_kst_date: datetime) -> Tuple[str, str]:
    start_kst = day_kst_date.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=KST)
    end_kst = start_kst + timedelta(days=1)
    start_utc = start_kst.astimezone(UTC)
    end_utc = end_kst.astimezone(UTC)
    return start_utc.isoformat().replace("+00:00", "Z"), end_utc.isoformat().replace("+00:00", "Z")


def pretty_kst_date(d: datetime) -> str:
    return d.astimezone(KST).strftime("%Y-%m-%d")


def auth_headers(token: str) -> Dict[str, str]:
    return {"Authorization": f"Bearer {token}"}


def ensure_ok(r: requests.Response) -> requests.Response:
    try:
        r.raise_for_status()
    except requests.HTTPError as e:
        msg = f"HTTP {r.status_code} for {r.request.method} {r.url}\nResponse: {r.text[:800]}"
        raise SystemExit(msg) from e
    return r


# ----- í”„ë¡œì íŠ¸ ID -----
def resolve_project_id(token: str, org: str, project_slug: Optional[str], project_id_env: Optional[str]) -> int:
    if project_id_env:
        return int(project_id_env)
    if not project_slug:
        raise SystemExit("SENTRY_PROJECT_SLUG ë˜ëŠ” SENTRY_PROJECT_ID ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤.")
    url = f"{API_BASE}/organizations/{org}/projects/"
    r = ensure_ok(requests.get(url, headers=auth_headers(token), timeout=30))
    for p in r.json():
        if p.get("slug") == project_slug:
            return int(p.get("id"))
    raise SystemExit(f"'{project_slug}' í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# ----- Discover ì§‘ê³„ (ì „ì²´ ìš”ì•½) -----
def discover_aggregates_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str
) -> Dict[str, Any]:
    url = f"{API_BASE}/organizations/{org}/events/"
    query = "level:[error,fatal]" + (f" environment:{environment}" if environment else "")
    params = {
        "field": ["count()", "count_unique(issue)", "count_unique(user)"],
        "project": project_id,
        "start": start_iso_utc,
        "end": end_iso_utc,
        "query": query,
        "referrer": "api.summaries.daily",
    }
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    rows = (r.json().get("data") or [])
    if not rows:
        return {"crash_events": 0, "unique_issues": 0, "impacted_users": 0}
    row0 = rows[0]
    return {
        "crash_events": int(row0.get("count()") or 0),
        "unique_issues": int(row0.get("count_unique(issue)") or 0),
        "impacted_users": int(row0.get("count_unique(user)") or 0),
    }


# ----- Discover: ì´ìŠˆë³„ count() ë§µ (í˜ì´ì§€ë„¤ì´ì…˜) -----
def parse_link_cursor(link_header: str) -> Optional[str]:
    if 'rel="next"' in link_header and 'results="true"' in link_header:
        try:
            start = link_header.index("cursor=") + len("cursor=")
            end = link_header.index(">", start)
            return link_header[start:end]
        except Exception:
            return None
    return None


def issue_counts_map_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str, per_page: int = 100, max_pages: int = 10
) -> Dict[str, Dict[str, Any]]:
    """
    ì§€ì • ì¼ìì˜ ì´ìŠˆë³„ count()ë¥¼ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ìµœëŒ€ max_pagesê¹Œì§€ ìˆ˜ì§‘
    ë°˜í™˜: { issue_id: {"count": int, "title": str} }
    """
    url = f"{API_BASE}/organizations/{org}/events/"
    query = "level:[error,fatal]" + (f" environment:{environment}" if environment else "")
    headers = auth_headers(token)

    out: Dict[str, Dict[str, Any]] = {}
    cursor = None
    page = 0
    while True:
        page += 1
        params = {
            "field": ["issue", "title", "count()"],
            "project": project_id,
            "start": start_iso_utc,
            "end": end_iso_utc,
            "query": query,
            "orderby": "-count()",
            "per_page": per_page,  # 1~100
            "referrer": "api.summaries.issue-counts",
        }
        if cursor:
            params["cursor"] = cursor
        r = ensure_ok(requests.get(url, headers=headers, params=params, timeout=60))
        rows = (r.json().get("data") or [])
        for row in rows:
            iid = row.get("issue")
            if not iid:
                continue
            out[str(iid)] = {
                "count": int(row.get("count()") or 0),
                "title": row.get("title"),
            }
        link = r.headers.get("link", "")
        cursor = parse_link_cursor(link)
        if not cursor or page >= max_pages:
            break
    return out


# ----- ìƒìœ„ 5ê°œ ì´ìŠˆ -----
def top_issues_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str, limit: int = 5
) -> List[Dict[str, Any]]:
    url = f"{API_BASE}/organizations/{org}/events/"
    query = "level:[error,fatal]" + (f" environment:{environment}" if environment else "")
    params = {
        "field": ["issue", "title", "count()"],
        "project": project_id,
        "start": start_iso_utc,
        "end": end_iso_utc,
        "query": query,
        "orderby": "-count()",
        "per_page": limit,
        "referrer": "api.summaries.top-issues",
    }
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    rows = (r.json().get("data") or [])
    return [
        {
            "issue_id": row.get("issue"),
            "title": row.get("title"),
            "event_count": row.get("count()"),
            "link": f"https://sentry.io/organizations/{org}/issues/{row.get('issue')}/" if row.get("issue") else None,
        }
        for row in rows[:limit]
    ]


# ----- ì‹ ê·œ ë°œìƒ ì´ìŠˆ (Issues API: firstSeen) -----
def new_issues_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str
) -> List[Dict[str, Any]]:
    url = f"{API_BASE}/organizations/{org}/issues/"
    q_parts = [f"firstSeen:>={start_iso_utc}", f"firstSeen:<{end_iso_utc}", "level:[error,fatal]"]
    if environment:
        q_parts.append(f"environment:{environment}")
    query = " ".join(q_parts)
    params = {
        "project": project_id,
        "since": start_iso_utc,
        "until": end_iso_utc,
        "query": query,
        "sort": "date",
        "per_page": 100,
        "referrer": "api.summaries.new-issues",
    }
    headers = auth_headers(token)
    results: List[Dict[str, Any]] = []
    cursor: Optional[str] = None

    while True:
        if cursor:
            params["cursor"] = cursor
        r = ensure_ok(requests.get(url, headers=headers, params=params, timeout=60))
        items = r.json() or []
        for it in items:
            iid = it.get("id")
            permalink = it.get("permalink") or (f"https://sentry.io/organizations/{org}/issues/{iid}/" if iid else None)
            results.append({
                "issue_id": iid,
                "title": it.get("title"),
                "event_count": int(it.get("count")) if it.get("count") is not None else None,
                "first_seen": it.get("firstSeen"),
                "link": permalink,
            })
        link = r.headers.get("link", "")
        cursor = parse_link_cursor(link)
        if not cursor:
            break

    return results


# ====== í†µê³„ ìœ í‹¸ ======
def mean_std(values: List[int]) -> Tuple[float, float]:
    if not values:
        return 0.0, 0.0
    m = sum(values) / len(values)
    var = sum((v - m) ** 2 for v in values) / max(len(values), 1)
    return m, math.sqrt(var)


def median(values: List[int]) -> float:
    if not values:
        return 0.0
    s = sorted(values)
    n = len(s)
    mid = n // 2
    if n % 2 == 1:
        return float(s[mid])
    return (s[mid - 1] + s[mid]) / 2.0


def mad(values: List[int], med: Optional[float] = None) -> float:
    if not values:
        return 0.0
    m = med if med is not None else median(values)
    dev = [abs(v - m) for v in values]
    return median(dev)


# ====== ê³ ê¸‰ ê¸‰ì¦ íƒì§€ ======
def detect_surge_issues_advanced(
    token: str, org: str, project_id: int, environment: Optional[str],
    target_start_utc: str, target_end_utc: str,
    baseline_days: int = BASELINE_DAYS,
    per_page: int = 100, max_pages: int = 10
) -> List[Dict[str, Any]]:
    # íƒ€ê²Ÿì¼ ì´ìŠˆë³„ ì¹´ìš´íŠ¸(í˜ì´ì§€ë„¤ì´ì…˜)
    today_map = issue_counts_map_for_day(
        token, org, project_id, environment,
        target_start_utc, target_end_utc, per_page, max_pages
    )

    # ì§ì „ Nì¼ ë§µë“¤
    def iso_to_dt(iso_str: str) -> datetime:
        return datetime.fromisoformat(iso_str.replace("Z", "+00:00")).astimezone(UTC)
    t_start_dt = iso_to_dt(target_start_utc)

    prev_maps: List[Dict[str, Dict[str, Any]]] = []
    for i in range(1, baseline_days + 1):
        day_start_dt = t_start_dt - timedelta(days=i)
        day_end_dt = day_start_dt + timedelta(days=1)
        start_iso = day_start_dt.isoformat().replace("+00:00", "Z")
        end_iso   = day_end_dt.isoformat().replace("+00:00", "Z")
        prev_maps.append(
            issue_counts_map_for_day(token, org, project_id, environment, start_iso, end_iso, per_page, max_pages)
        )

    results: List[Dict[str, Any]] = []
    eps = 1e-9

    for iid, cur_info in today_map.items():
        # ë°©ì–´: íƒ€ì… ì„ì„ ëŒ€ë¹„
        try:
            cur = int(cur_info.get("count") or 0)
        except Exception:
            cur = 0

        # 1ì°¨ í•„í„°: ì ˆëŒ€ ìµœì†Œ ê±´ìˆ˜(ì–´ë–¤ ì¡°ê±´ì´ë“  ì´ ê°’ ë¯¸ë§Œì´ë©´ ì œì™¸)
        if cur < SURGE_ABSOLUTE_MIN:
            continue

        title = cur_info.get("title")
        link  = f"https://sentry.io/organizations/{org}/issues/{iid}/"

        # D-1 ë° ë² ì´ìŠ¤ë¼ì¸
        dby = int(prev_maps[0].get(iid, {}).get("count") or 0) if prev_maps else 0
        baseline_counts = [int(pm.get(iid, {}).get("count") or 0) for pm in prev_maps]

        mean_val, std_val = mean_std(baseline_counts)
        med_val           = median(baseline_counts)
        mad_val           = mad(baseline_counts, med_val)

        z = (cur - mean_val) / (std_val + eps) if std_val > 0 else (float("inf") if cur > mean_val else 0.0)
        mad_score = (cur - med_val) / (1.4826 * mad_val + eps) if mad_val > 0 else (float("inf") if cur > med_val else 0.0)
        growth = cur / max(dby, 1)

        is_all_zero = all(v == 0 for v in baseline_counts)
        conditions = {
            "growth":   growth >= SURGE_GROWTH_MULTIPLIER,
            "zscore":   z >= SURGE_Z_THRESHOLD,
            "madscore": mad_score >= SURGE_MAD_THRESHOLD,
            # ì™„ì „ ì‹ ê·œ í­ë°œ: ê·¸ë˜ë„ curëŠ” ìœ„ì˜ SURGE_ABSOLUTE_MINì„ ì´ë¯¸ í†µê³¼í•´ì•¼ í•¨
            "new_burst": (is_all_zero and cur >= max(SURGE_MIN_NEW_BURST, SURGE_ABSOLUTE_MIN)),
        }

        if any(conditions.values()):
            results.append({
                "issue_id": iid,
                "title": title,
                "event_count": cur,
                "link": link,
                "dby_count": dby,
                "growth_multiplier": round(growth, 2),
                "zscore": None if math.isinf(z) else round(z, 2),
                "mad_score": None if math.isinf(mad_score) else round(mad_score, 2),
                "baseline_mean": round(mean_val, 2),
                "baseline_std": round(std_val, 2),
                "baseline_median": round(med_val, 2),
                "baseline_mad": round(mad_val, 2),
                "baseline_counts": baseline_counts,
                "reasons": [k for k, v in conditions.items() if v],
            })

    # 2ì°¨ ë³´ì •: í˜¹ì‹œë¼ë„ ê³„ì‚°/íƒ€ì… ì´ìŠˆë¡œ í†µê³¼í•œ í•­ëª©ì„ ë‹¤ì‹œ ì ˆëŒ€ ìµœì†Œê±´ìˆ˜ë¡œ ê±¸ëŸ¬ëƒ„
    results = [r for r in results if int(r.get("event_count") or 0) >= SURGE_ABSOLUTE_MIN]

    # ì •ë ¬/ìƒí•œ
    results.sort(
        key=lambda x: (x["event_count"], (x["zscore"] or 0), (x["mad_score"] or 0), x["growth_multiplier"]),
        reverse=True
    )
    return results[:SURGE_MAX_RESULTS]


# ----- Crash Free ë©”íŠ¸ë¦­ -----
def sessions_crash_free_for_day(
    token: str, org: str, project_id: int, environment: Optional[str],
    start_iso_utc: str, end_iso_utc: str
) -> Tuple[Optional[float], Optional[float]]:
    url = f"{API_BASE}/organizations/{org}/sessions/"
    params = {
        "project": project_id,
        "start": start_iso_utc,
        "end": end_iso_utc,
        "interval": "1d",
        "field": ["crash_free_rate(session)", "crash_free_rate(user)"],
        "referrer": "api.summaries.daily",
    }
    if environment:
        params["environment"] = environment
    r = ensure_ok(requests.get(url, headers=auth_headers(token), params=params, timeout=60))
    data = r.json()
    cf_s, cf_u = None, None
    for g in data.get("groups", []):
        series = g.get("series", {})
        if "crash_free_rate(session)" in series and series["crash_free_rate(session)"]:
            cf_s = float(series["crash_free_rate(session)"][-1])
        if "crash_free_rate(user)" in series and series["crash_free_rate(user)"]:
            cf_u = float(series["crash_free_rate(user)"][-1])
    return cf_s, cf_u


# =========================
# AI ì¡°ì–¸ ìƒì„± (OpenAI)
# =========================
from openai import OpenAI
import re

def generate_ai_advice(summary_payload: Dict[str, Any], y_key: str, dby_key: Optional[str], environment: Optional[str]) -> Dict[str, Any]:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return {"fallback_text": "AI ì¡°ì–¸ì„ ìƒì„±í•˜ë ¤ë©´ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤."}

    client = OpenAI(api_key=api_key)

    # ì–´ì œ ìƒìœ„ 5 ì´ìŠˆ(ê°„ë‹¨ ë²„ì „)ë¥¼ ë³„ë„ë¡œ ì œê³µ â†’ per_issue_notes ë§¤ì¹­ ì •í™•ë„ â†‘
    top5 = (summary_payload.get(y_key, {}) or {}).get("top_5_issues", []) or []
    top5_compact = [
        {"issue_id": t.get("issue_id"), "title": t.get("title"), "event_count": t.get("event_count")}
        for t in top5
    ]

    # âœ… ë§¥ë½/ì •ì˜ ì„¤ëª… + ìš”ì•½ ë°ì´í„°(JSON) ê·¸ëŒ€ë¡œ ì „ë‹¬ (ê°„ë‹¨í•˜ì§€ë§Œ ì¶©ë¶„í•œ ì»¨í…ìŠ¤íŠ¸)
    prompt = (
        "ë‹¹ì‹ ì€ Sentry ì˜¤ë¥˜ ë¦¬í¬íŠ¸ë¥¼ ë¶„ì„í•˜ëŠ” ì¹œê·¼í•œ AI ì½”ì¹˜ì…ë‹ˆë‹¤. "
        "ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ, ê°œë°œìì—ê²Œ ë„ì›€ì´ ë  ì¸ì‚¬ì´íŠ¸ë¥¼ ì£¼ì„¸ìš”. "
        "ë°ì´í„°ë¥¼ ê·¸ëƒ¥ ë‚˜ì—´í•˜ì§€ ë§ê³  ì˜ë¯¸ ìˆëŠ” í¬ì¸íŠ¸ë§Œ ë½‘ì•„ì£¼ì„¸ìš”. ì¸ì‚¬ì´íŠ¸ê°€ ì—†ë‹¤ë©´ ê°„ë‹¨íˆ ë„˜ì–´ê°€ë„ ë©ë‹ˆë‹¤. "
        "ë§ˆì§€ë§‰ì—” ì˜¤ëŠ˜ ìƒí™©ì„ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì¹œê·¼í•˜ê²Œ ìš”ì•½(ê°€ë²¼ìš´ ë†ë‹´/ê²©ë ¤ í—ˆìš©, ì¤‘ìš”í•œ ê²½ê³ ëŠ” ë¶„ëª…íˆ)í•˜ì„¸ìš”.\n\n"
        "=== ë¶„ì„ ë§¥ë½ ===\n"
        f"- ì´ ë¦¬í¬íŠ¸ëŠ” 'ì–´ì œ({y_key})' ê¸°ì¤€ Summary ë°ì´í„°ì…ë‹ˆë‹¤. ë¹„êµ ëŒ€ìƒì€ 'ê·¸ì €ê»˜({dby_key or 'N/A'})'ì…ë‹ˆë‹¤.\n"
        "- ì´ë²¤íŠ¸(crash_events): levelì´ fatal, errorì¸ ì´ë²¤íŠ¸ ë°œìƒ ê±´ìˆ˜ì…ë‹ˆë‹¤.\n"
        "- ì´ìŠˆ(unique_issues): ìœ„ ì´ë²¤íŠ¸ê°€ ì†í•œ ê³ ìœ  ì´ìŠˆ ê°œìˆ˜ì…ë‹ˆë‹¤.\n"
        "- ì˜í–¥ ì‚¬ìš©ì(impacted_users): í•´ë‹¹ ì´ìŠˆë¡œ ì˜í–¥ì„ ë°›ì€ ì‚¬ìš©ì ìˆ˜ì…ë‹ˆë‹¤.\n"
        "- Crash Free: 'crash_free_rate(session/user)'ë¡œ, ë†’ì€ ê°’ì¼ìˆ˜ë¡ ì•ˆì •ì ì…ë‹ˆë‹¤.\n\n"
        "=== ì¶œë ¥ í˜•ì‹ ===\n"
        "ë°˜ë“œì‹œ **ìˆœìˆ˜ JSONë§Œ** ì¶œë ¥í•˜ì„¸ìš”. ì½”ë“œë¸”ë¡(````json`)ë¡œ ê°ì‹¸ì§€ ë§ˆì„¸ìš”.\n"
        "{\n"
        "  \"newsletter_summary\": \"\",   \n"
        "  \"today_actions\": [ {\"title\":\"\", \"why\":\"\", \"owner_role\":\"\", \"suggestion\":\"\"} ],\n"
        "  \"root_cause\": [],\n"
        "  \"per_issue_notes\": [ {\"issue_title\":\"\", \"note\":\"\"} ]\n"
        "}\n\n"
        "=== per_issue_notes ì‘ì„± ê·œì¹™ ===\n"
        "- ì•„ë˜ ìƒìœ„ 5ê°œ ì´ìŠˆ ì¤‘ **ì˜ë¯¸ ìˆëŠ” í•­ëª©ë§Œ** ì½”ë©˜íŠ¸ë¥¼ ë‚¨ê¸°ì„¸ìš”(ë¶ˆí•„ìš”í•˜ë©´ ìƒëµ ê°€ëŠ¥).\n"
        "- `issue_title`ì€ ë°˜ë“œì‹œ ì•„ë˜ title ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”(ë³€í˜• ê¸ˆì§€).\n\n"
        f"[ìƒìœ„ 5ê°œ ì´ìŠˆ (ìš”ì•½)]\n{json.dumps(top5_compact, ensure_ascii=False)}\n\n"
        "=== ì „ì²´ Summary JSON ===\n"
        f"{json.dumps(summary_payload, ensure_ascii=False)}\n"
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=900,
            messages=[{"role": "user", "content": prompt}],
        )
        text = resp.choices[0].message.content.strip()

        # ëª¨ë¸ì´ í˜¹ì‹œ ì½”ë“œë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ë©´ JSONë§Œ ì¶”ì¶œ
        if text.startswith("```"):
            m = re.search(r"\{.*\}", text, re.DOTALL)
            if m:
                text = m.group(0)

        data = json.loads(text)

        # ê¸°ë³¸ í‚¤/íƒ€ì… ë³´ì •
        data.setdefault("newsletter_summary", "")
        data.setdefault("today_actions", [])
        data.setdefault("root_cause", [])
        data.setdefault("per_issue_notes", [])

        if not isinstance(data["newsletter_summary"], str):
            data["newsletter_summary"] = str(data["newsletter_summary"])
        for k in ["today_actions", "root_cause", "per_issue_notes"]:
            if not isinstance(data[k], list):
                data[k] = []

        # ì•¡ì…˜ í•­ëª© ì •ê·œí™”
        norm_actions = []
        for x in data["today_actions"]:
            if isinstance(x, dict):
                a = {
                    "title": (x.get("title") or "").strip(),
                    "why": (x.get("why") or "").strip(),
                    "owner_role": (x.get("owner_role") or "").strip(),
                    "suggestion": (x.get("suggestion") or "").strip(),
                }
                if any(a.values()):
                    norm_actions.append(a)
        data["today_actions"] = norm_actions

        # per_issue_notes ë¬¸ìì—´ ì •ê·œí™”
        data["per_issue_notes"] = [
            {"issue_title": str(i.get("issue_title", "")).strip(), "note": str(i.get("note", "")).strip()}
            for i in data["per_issue_notes"] if isinstance(i, dict)
        ]

        # (ì˜µì…˜) ìµœì†Œ í´ë°±: per_issue_notesê°€ ë¹„ì—ˆê³  root_causeê°€ ìˆìœ¼ë©´ top1ì— í•œ ì¤„ ë¶™ì—¬ì¤Œ
        if not data["per_issue_notes"]:
            root_causes = data.get("root_cause") or []
            if top5 and root_causes:
                data["per_issue_notes"] = [{
                    "issue_title": top5[0].get("title") or "(ì œëª© ì—†ìŒ)",
                    "note": str(root_causes[0]).strip()
                }]

        return data

    except Exception as e:
        snippet = text[:200] if 'text' in locals() and text else "(no text)"
        return {"fallback_text": f"AI ì¡°ì–¸ ìƒì„± ì‹¤íŒ¨: {e.__class__.__name__}: {str(e)} | snippet={snippet}"}


# ====== Slack ë©”ì‹œì§€ ë¹Œë”/ì „ì†¡ ======
# ---------- ë„ìš°ë¯¸ ----------
def truncate(s: Optional[str], n: int) -> Optional[str]:
    if s is None:
        return None
    return s if len(s) <= n else s[: n - 1] + "â€¦"

def fmt_pct(v: Optional[float]) -> str:
    if v is None:
        return "N/A"
    pct = v * 100
    truncated = int(pct * 100) / 100  # ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ ì ˆì‚­
    return f"{truncated:.2f}%"

def parse_iso_to_kst_label(start_utc_iso: str, end_utc_iso: str) -> str:
    """UTC ISO êµ¬ê°„ì„ í•œêµ­ì‹œ(KST)ë¡œ ë°”ê¿” ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ í‘œê¸°"""
    def to_kst(iso_s: str) -> datetime:
        return datetime.fromisoformat(iso_s.replace("Z", "+00:00")).astimezone(KST)
    s = to_kst(start_utc_iso)
    e = to_kst(end_utc_iso)
    # ì˜ˆ: 2025-09-01 00:00 ~ 2025-09-01 23:59 (KST)
    s_txt = s.strftime("%Y-%m-%d %H:%M")
    e_txt = e.strftime("%Y-%m-%d %H:%M")
    return f"{s_txt} ~ {e_txt} (KST)"

def diff_str(cur: int, prev: int, suffix: str = "ê±´") -> str:
    delta = cur - prev
    if delta > 0:
        arrow = "ğŸ”º"
    elif delta < 0:
        arrow = "ğŸ”»"
    else:
        arrow = "â€”"
    ratio = ""
    if prev > 0:
        ratio = f" ({(delta/prev)*100:+.1f}%)"
    return f"{cur}{suffix} {arrow}{abs(delta)}{suffix}{ratio}"

# ---------- ì´ìŠˆ ë¼ì¸(í•œêµ­ì–´) ----------
def issue_line_kr(item: Dict[str, Any]) -> str:
    """ì œëª©ì—ë§Œ ë§í¬, ì´ìŠˆí‚¤(#... ) ì œê±°, ê°œìˆ˜ëŠ” '7ê±´'ìœ¼ë¡œ í‘œê¸°"""
    title = truncate(item.get("title"), TITLE_MAX) or "(ì œëª© ì—†ìŒ)"
    link = item.get("link")
    count = item.get("event_count")
    count_txt = f"{int(count)}ê±´" if isinstance(count, (int, float)) and count is not None else "â€“"
    title_link = f"<{link}|{title}>" if link else title
    return f"â€¢ {title_link} Â· {count_txt}"

# ---------- ê¸‰ì¦ ì´ìŠˆ ì„¤ëª…(ì„œìˆ í˜•) ----------
def surge_explanation_kr(item: Dict[str, Any]) -> str:
    """
    ì˜ˆì‹œ:
    â€¢ Login NPE Â· 42ê±´
      â†³ ì „ì¼ 0ê±´ â†’ ì–´ì œ 42ê±´ìœ¼ë¡œ ê¸‰ì¦. ìµœê·¼ 7ì¼ í‰ê·  5.3ê±´/ì¤‘ì•™ê°’ 4ê±´ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€.
      â†³ íŒì • ê·¼ê±°: growth/madscore
    """
    base = issue_line_kr(item)
    cur = item.get("event_count") or 0
    d1 = item.get("dby_count") or 0
    mean_v = item.get("baseline_mean")
    med_v = item.get("baseline_median")
    reasons = item.get("reasons", [])
    # ì„œìˆ : ì „ì¼ ëŒ€ë¹„, 7ì¼ í‰ê· /ì¤‘ì•™ê°’ ëŒ€ë¹„
    parts = []
    parts.append(f"ì „ì¼ {d1}ê±´ â†’ ì–´ì œ {cur}ê±´ìœ¼ë¡œ ê¸‰ì¦.")
    if isinstance(mean_v, (int, float)) and isinstance(med_v, (int, float)):
        parts.append(f"ìµœê·¼ 7ì¼ í‰ê·  {mean_v:.1f}ê±´/ì¤‘ì•™ê°’ {med_v:.0f}ê±´ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€.")
    # ê·œì¹™ëª…ë§Œ ê°„ë‹¨ í‘œê¸°
    if reasons:
        ko = {
            "growth": "ì „ì¼ ëŒ€ë¹„ ê¸‰ì¦",
            "zscore": "í‰ê·  ëŒ€ë¹„ í†µê³„ì  ê¸‰ì¦",
            "madscore": "ì¤‘ì•™ê°’ ëŒ€ë¹„ ì´ìƒì¹˜",
            "new_burst": "ìµœê·¼ ê¸°ë¡ ê±°ì˜ ì—†ìŒì—ì„œ í­ì¦",
        }
        pretty = [ko.get(r, r) for r in reasons]
        parts.append("íŒì • ê·¼ê±°: " + "/".join(pretty))
    detail = "  â†³ " + " ".join(parts)
    return f"{base}\n{detail}"

# ---------- í•œêµ­ì–´ ë¸”ë¡ ë¹Œë” ----------
def build_ai_advice_blocks(ai: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    AI ê²°ê³¼ dict â†’ Slack Blocks
    - íƒ€ì´í‹€: ":brain: AI ë¶„ì„ ì½”ë©˜íŠ¸"
    - ë‚´ìš©: ë‰´ìŠ¤ë ˆí„° ìš”ì•½(í•„ìˆ˜) + ì˜¤ëŠ˜ì˜ ì•¡ì…˜(ì˜µì…˜)
    - ì›ì¸ ì¶”ì •Â·ì ê²€ / ì´ìŠˆë³„ ì½”ë©˜íŠ¸ëŠ” top5 ì„¹ì…˜ì— ë³‘í•©ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŒ
    """
    blocks: List[Dict[str, Any]] = []
    blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:brain: AI ë¶„ì„ ì½”ë©˜íŠ¸*"}})

    if "fallback_text" in ai:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": ai["fallback_text"]}})
        blocks.append({"type": "divider"})
        return blocks

    summary_text = (ai.get("newsletter_summary") or "").strip()
    if summary_text:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": f"> {summary_text}"}})

    actions = ai.get("today_actions", []) or []
    if actions:
        lines = []
        for x in actions:
            t = x.get("title", "").strip() or "(ì œëª© ì—†ìŒ)"
            s = x.get("suggestion", "").strip()
            extra = []
            if x.get("owner_role"): extra.append(f"ë‹´ë‹¹: {x['owner_role']}")
            if x.get("why"):        extra.append(f"ì´ìœ : {x['why']}")
            suffix = f" _({', '.join(extra)})_" if extra else ""
            lines.append(f"â€¢ *{t}* â€” {s}{suffix}")
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*ì˜¤ëŠ˜ì˜ ì•¡ì…˜*\n" + "\n".join(lines)}})

    blocks.append({"type": "divider"})
    return blocks

def build_slack_blocks_for_day(
    date_label: str,
    env_label: Optional[str],
    day_obj: Dict[str, Any],
    prev_day_obj: Optional[Dict[str, Any]] = None,
    ai_blocks: Optional[List[Dict[str, Any]]] = None,
    ai_data: Optional[Dict[str, Any]] = None,
) -> List[Dict[str, Any]]:
    # í˜„ì¬ê°’
    cf_s = day_obj.get("crash_free_sessions_pct")
    cf_u = day_obj.get("crash_free_users_pct")
    events = int(day_obj.get("crash_events", 0))
    issues = int(day_obj.get("unique_issues", 0))
    users  = int(day_obj.get("impacted_users", 0))

    # ì „ì¼ê°’ (ì¦ê°ì€ ì´ë²¤íŠ¸/ì´ìŠˆ/ì‚¬ìš©ìì—ë§Œ ì ìš©)
    prev_events = prev_issues = prev_users = 0
    if prev_day_obj:
        prev_events = int(prev_day_obj.get("crash_events", 0))
        prev_issues = int(prev_day_obj.get("unique_issues", 0))
        prev_users  = int(prev_day_obj.get("impacted_users", 0))

    # Summary: ìš”ì²­í•˜ì‹  ìˆœì„œë¡œ í‘œê¸° (ì´ë²¤íŠ¸/ì´ìŠˆ/ì‚¬ìš©ì â†’ Crash Free)
    summary_lines = [
        "*:memo: Summary*",
        f"â€¢ ğŸ’¥ *ì´ë²¤íŠ¸*: {diff_str(events, prev_events, suffix='ê±´') if prev_day_obj else f'{events}ê±´'}",
        f"â€¢ ğŸ *ì´ìŠˆ*: {diff_str(issues, prev_issues, suffix='ê±´') if prev_day_obj else f'{issues}ê±´'}",
        f"â€¢ ğŸ‘¥ *ì˜í–¥ ì‚¬ìš©ì*: {diff_str(users, prev_users, suffix='ëª…') if prev_day_obj else f'{users}ëª…'}",
        f"â€¢ ğŸ›¡ï¸ *Crash Free ì„¸ì…˜*: {fmt_pct(cf_s)} / *Crash Free ì‚¬ìš©ì*: {fmt_pct(cf_u)}",
    ]
    kpi_text = "\n".join(summary_lines)

    # ì§‘ê³„ êµ¬ê°„(KST)
    win = day_obj.get("window_utc") or {}
    kst_window = parse_iso_to_kst_label(win.get("start","?"), win.get("end","?"))

    # í—¤ë”
    title = f"Sentry ì¼ê°„ ë¦¬í¬íŠ¸ â€” {date_label}"
    if env_label:
        title += f"  Â·  {env_label}"

    blocks: List[Dict[str, Any]] = [
        {"type": "header", "text": {"type": "plain_text", "text": title, "emoji": True}},
        {"type": "section", "text": {"type": "mrkdwn", "text": kpi_text}},
        {"type": "context", "elements": [{"type": "mrkdwn", "text": f"*ì§‘ê³„ êµ¬ê°„*: {kst_window}"}]},
        {"type": "divider"},
    ]

    # === ì—¬ê¸°ì„œ AI ì„¹ì…˜ ì‚½ì… ===
    if ai_blocks:
        blocks.extend(ai_blocks)

    # ì•„ë˜ëŠ” ê¸°ì¡´ ì„¹ì…˜: íƒ€ì´í‹€ì€ ì´ëª¨ì§€ + êµµê²Œ ìœ ì§€
    top = day_obj.get("top_5_issues") or []
    if top:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:sports_medal: ìƒìœ„ 5ê°œ ì´ìŠˆ*"}})
        merged_lines = render_top5_with_ai(top, ai_data) if ai_data else "\n".join(issue_line_kr(x) for x in top)
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": merged_lines}})
        blocks.append({"type": "divider"})

    new_issues = (day_obj.get("new_issues") or [])[:SLACK_MAX_NEW]
    if new_issues:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:new: ì‹ ê·œ ë°œìƒ ì´ìŠˆ*"}})
        lines = "\n".join(issue_line_kr(x) for x in new_issues)
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": lines}})
        blocks.append({"type": "divider"})

    surge = [x for x in (day_obj.get("surge_issues") or []) if int(x.get("event_count") or 0) >= SURGE_ABSOLUTE_MIN]
    surge = surge[:SLACK_MAX_SURGE]
    if surge:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*:chart_with_upwards_trend: ê¸‰ì¦(ì„œì§€) ì´ìŠˆ*"}})
        lines = "\n".join(surge_explanation_kr(x) for x in surge)
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": lines}})
        blocks.append({"type": "divider"})

    return blocks

def _norm_title(s: str) -> str:
    return " ".join((s or "").lower().strip().replace("â€¦", "").split())

def render_top5_with_ai(top5: List[Dict[str, Any]], ai: Dict[str, Any]) -> str:
    """
    ìƒìœ„ 5ê°œ ì´ìŠˆ ë¼ì¸ì— AIì˜ per_issue_notesë¥¼ ë“¤ì—¬ì“´ ë¶ˆë¦¿ìœ¼ë¡œ ê²°í•©.
    ìš°ì„ ìˆœìœ„: issue_id ë§¤ì¹­ > ì œëª© ì •ê·œí™” ë§¤ì¹­(startswith í¬í•¨)
    ì¶œë ¥ì€ mrkdwn ë¬¸ìì—´.
    """
    notes = ai.get("per_issue_notes") or []
    # ì¸ë±ìŠ¤: issue_id â†’ [notes], title_norm â†’ [notes]
    by_id: Dict[str, List[Dict[str, Any]]] = {}
    by_tn: Dict[str, List[Dict[str, Any]]] = {}
    for n in notes:
        if not isinstance(n, dict): continue
        iid = str(n.get("issue_id") or "").strip()
        ititle = n.get("issue_title") or ""
        tn = _norm_title(ititle)
        by_id.setdefault(iid, []).append(n) if iid else None
        if tn:
            by_tn.setdefault(tn, []).append(n)

    lines: List[str] = []
    for it in top5:
        title = truncate(it.get("title"), TITLE_MAX) or "(ì œëª© ì—†ìŒ)"
        link  = it.get("link")
        cnt   = it.get("event_count")
        cnt_t = f"{int(cnt)}ê±´" if isinstance(cnt, (int, float)) and cnt is not None else "â€“"
        head  = f"â€¢ <{link}|{title}> Â· {cnt_t}" if link else f"â€¢ {title} Â· {cnt_t}"
        lines.append(head)

        # ë§¤ì¹­ ë…¸íŠ¸ ìˆ˜ì§‘
        matched: List[Dict[str, Any]] = []
        iid = str(it.get("issue_id") or "").strip()
        if iid and iid in by_id:
            matched.extend(by_id[iid])
        else:
            tn = _norm_title(title)
            # exact ìš°ì„ , ì—†ìœ¼ë©´ startswith ìœ ì‚¬ ë§¤ì¹­
            if tn in by_tn:
                matched.extend(by_tn[tn])
            else:
                # ëŠìŠ¨í•œ startswith
                for k, v in by_tn.items():
                    if tn.startswith(k) or k.startswith(tn):
                        matched.extend(v)
                        break

        # ë“¤ì—¬ì“´ ë¶ˆë¦¿(ìˆì„ ë•Œë§Œ)
        for n in matched:
            note = (n.get("note") or "").strip()
            cause = (n.get("why") or n.get("root_cause") or "").strip()  # í˜¹ì‹œ why/root_cause í•„ë“œê°€ ì˜¨ ê²½ìš° ëŒ€ë¹„
            if cause:
                lines.append(f"  â—¦ ì›ì¸/ì ê²€: {cause}")
            if note:
                lines.append(f"  â—¦ ì½”ë©˜íŠ¸: {note}")

    return "\n".join(lines)

# ---------- Slack ì „ì†¡ ----------
def post_to_slack(webhook_url: str, blocks: List[Dict[str, Any]]) -> None:
    payload = {"blocks": blocks}
    r = requests.post(webhook_url, headers={"Content-Type": "application/json"}, data=json.dumps(payload), timeout=30)
    try:
        r.raise_for_status()
    except requests.HTTPError as e:
        print(f"[Slack] Post failed {r.status_code}: {r.text[:300]}")
        raise


# ====== ë©”ì¸ ======
def main():
    step_total = 14
    log(f"[1/{step_total}] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œâ€¦")
    load_dotenv()
    token = os.getenv("SENTRY_AUTH_TOKEN") or ""
    org = os.getenv("SENTRY_ORG_SLUG") or ""
    project_slug = os.getenv("SENTRY_PROJECT_SLUG")
    project_id_env = os.getenv("SENTRY_PROJECT_ID")
    environment = os.getenv("SENTRY_ENVIRONMENT")
    slack_webhook = os.getenv("SLACK_WEBHOOK_URL")

    if not token or not org:
        raise SystemExit("SENTRY_AUTH_TOKEN, SENTRY_ORG_SLUG í•„ìˆ˜")

    log(f"[2/{step_total}] ë‚ ì§œ ê³„ì‚°(KST ê¸°ì¤€ ì–´ì œ/ê·¸ì €ê»˜)â€¦")
    now_kst = datetime.now(KST)
    y_kst = (now_kst - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    dby_kst = (now_kst - timedelta(days=2)).replace(hour=0, minute=0, second=0, microsecond=0)
    y_start, y_end = kst_day_bounds_utc_iso(y_kst)
    dby_start, dby_end = kst_day_bounds_utc_iso(dby_kst)
    log(f"  - ì–´ì œ(KST): {pretty_kst_date(y_kst)} / UTC: {y_start} ~ {y_end}")
    log(f"  - ê·¸ì €ê»˜(KST): {pretty_kst_date(dby_kst)} / UTC: {dby_start} ~ {dby_end}")

    log(f"[3/{step_total}] í”„ë¡œì íŠ¸ í™•ì¸/í•´ê²°(org={org}, slug={project_slug}, id_env={project_id_env})â€¦")
    project_id = resolve_project_id(token, org, project_slug, project_id_env)
    log(f"  - project_id={project_id}")

    # --- ì–´ì œ ë°ì´í„° ---
    log(f"[4/{step_total}] ì–´ì œ ì§‘ê³„ ìˆ˜ì§‘(count/unique issue/user)â€¦")
    y_summary = discover_aggregates_for_day(token, org, project_id, environment, y_start, y_end)
    log(f"  - events={y_summary.get('crash_events')} / issues={y_summary.get('unique_issues')} / users={y_summary.get('impacted_users')}")

    log(f"[5/{step_total}] ì–´ì œ Crash Free(session/user) ìˆ˜ì§‘â€¦")
    y_cf_s, y_cf_u = sessions_crash_free_for_day(token, org, project_id, environment, y_start, y_end)
    log(f"  - crash_free(session)={fmt_pct(y_cf_s)} / crash_free(user)={fmt_pct(y_cf_u)}")

    log(f"[6/{step_total}] ì–´ì œ ìƒìœ„ 5ê°œ ì´ìŠˆ ìˆ˜ì§‘â€¦")
    y_top = top_issues_for_day(token, org, project_id, environment, y_start, y_end)
    log(f"  - top5 count={len(y_top)}")

    log(f"[7/{step_total}] ì–´ì œ ì‹ ê·œ ë°œìƒ ì´ìŠˆ(firstSeen ë‹¹ì¼) ìˆ˜ì§‘â€¦")
    y_new = new_issues_for_day(token, org, project_id, environment, y_start, y_end)
    log(f"  - new issues count={len(y_new)}")

    log(f"[8/{step_total}] ì–´ì œ ê¸‰ì¦(ì„œì§€) ì´ìŠˆ íƒì§€(ë² ì´ìŠ¤ë¼ì¸ {BASELINE_DAYS}ì¼)â€¦")
    y_surge_adv = detect_surge_issues_advanced(token, org, project_id, environment, y_start, y_end)
    log(f"  - surge detected={len(y_surge_adv)} (min_count={SURGE_MIN_COUNT})")

    # --- ê·¸ì €ê»˜ ë°ì´í„° (ë¹„êµìš©/ì¶œë ¥ í¬í•¨) ---
    log(f"[9/{step_total}] ê·¸ì €ê»˜ ì§‘ê³„ ìˆ˜ì§‘â€¦")
    dby_summary = discover_aggregates_for_day(token, org, project_id, environment, dby_start, dby_end)
    log(f"  - events={dby_summary.get('crash_events')} / issues={dby_summary.get('unique_issues')} / users={dby_summary.get('impacted_users')}")

    log(f"[10/{step_total}] ê·¸ì €ê»˜ Crash Free(session/user) ìˆ˜ì§‘â€¦")
    dby_cf_s, dby_cf_u = sessions_crash_free_for_day(token, org, project_id, environment, dby_start, dby_end)
    log(f"  - crash_free(session)={fmt_pct(dby_cf_s)} / crash_free(user)={fmt_pct(dby_cf_u)}")

    result = {
        "timezone": "Asia/Seoul (KST)",
        pretty_kst_date(y_kst): {
            **y_summary,
            "issues_count": y_summary["unique_issues"],
            "unique_issues_in_events": y_summary["unique_issues"],
            "crash_free_sessions_pct": y_cf_s,
            "crash_free_users_pct": y_cf_u,
            "top_5_issues": y_top,
            "new_issues": y_new,
            "surge_issues": y_surge_adv,
            "window_utc": {"start": y_start, "end": y_end},
        },
        pretty_kst_date(dby_kst): {
            **dby_summary,
            "issues_count": dby_summary["unique_issues"],
            "unique_issues_in_events": dby_summary["unique_issues"],
            "crash_free_sessions_pct": dby_cf_s,
            "crash_free_users_pct": dby_cf_u,
            "window_utc": {"start": dby_start, "end": dby_end},
        },
    }

    log(f"[11/{step_total}] ì½˜ì†” ì¶œë ¥(JSON)â€¦")
    print(json.dumps(result, ensure_ascii=False, indent=2))

    if slack_webhook:
        y_key = pretty_kst_date(y_kst)
        dby_key = pretty_kst_date(dby_kst)

        log(f"[12/{step_total}] AI ì½”ë©˜íŠ¸ ìƒì„± ì‹œë„(gpt-4o-mini)â€¦")
        ai_data = generate_ai_advice(result, y_key=y_key, dby_key=dby_key, environment=environment)
        if "fallback_text" in ai_data:
            log(f"  - AI ìƒì„± ì‹¤íŒ¨: {ai_data['fallback_text']}")
        else:
            log("  - AI ì½”ë©˜íŠ¸ ìƒì„± ì™„ë£Œ")
        ai_blocks = build_ai_advice_blocks(ai_data)

        log(f"[13/{step_total}] Slack Blocks êµ¬ì¶•â€¦")
        y_blocks = build_slack_blocks_for_day(
            date_label=y_key,
            env_label=environment,
            day_obj=result[y_key],
            prev_day_obj=result.get(dby_key),
            ai_blocks=ai_blocks,
            ai_data=ai_data,
        )

        log(f"[14/{step_total}] Slack ì „ì†¡ ì‹œë„â€¦")
        try:
            post_to_slack(slack_webhook, y_blocks)
            log("  - ì „ì†¡ ì™„ë£Œ âœ…")
        except Exception as e:
            log(f"  - ì „ì†¡ ì‹¤íŒ¨ âŒ: {e}")
    else:
        log("Slack Webhook ë¯¸ì„¤ì • â€” ì „ì†¡ ìŠ¤í‚µ")

if __name__ == "__main__":
    main()